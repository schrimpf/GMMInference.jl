<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Estimation - GMMInference.jl</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">GMMInference.jl</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Package Documentation</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Notes and Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active">Estimation</a>
</li>
                                    
<li>
    <a href="../identificationRobustInference/" class="dropdown-item">Inference</a>
</li>
                                    
<li>
    <a href="../empiricalLikelihood/" class="dropdown-item">Empirical Likelihood</a>
</li>
                                    
<li>
    <a href="../bootstrap/" class="dropdown-item">Bootstrap</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                            <li class="nav-item">
                                <a rel="prev" href=".." class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../identificationRobustInference/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/GMMInference.jl/edit/master/docs/extremumEstimation.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            
            <li class="nav-item" data-level="1"><a href="#extremum-estimation" class="nav-link">Extremum Estimation</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#example-logit" class="nav-link">Example: logit</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#review-of-extremum-estimator-theory" class="nav-link">Review of extremum estimator theory</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#consistency" class="nav-link">Consistency</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#asymptotic-normality" class="nav-link">Asymptotic normality</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#delta-method" class="nav-link">Delta method</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#references" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<h3 -="-" id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in
<a href="https://github.com/schrimpf/GMMInference.jl">on github</a>. The same
document generates both static webpages and associated <a href="../extremumEstimation.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,}
\def\indist{\,{\buildrel d \over \rightarrow}\,}
</script>
</p>
<h1 id="extremum-estimation">Extremum Estimation<a class="headerlink" href="#extremum-estimation" title="Permanent link">&para;</a></h1>
<p>Many, perhaps most, estimators in econometrics are extrumem
estimators. That is, many estimators are defined by</p>
<p>
<script type="math/tex; mode=display">
\hat{\theta} = \argmax_{\theta \in \Theta}
\hat{Q}_n(\theta)
</script>
</p>
<p>where $\hat{Q}_n(\theta)$ is some objective
function that depends on data. Examples include maximum likelihood,</p>
<p>
<script type="math/tex; mode=display">
\hat{Q}_n(\theta) = \frac{1}{n} \sum_{i=1}^n f(z_i | \theta)
</script>
</p>
<p>GMM,</p>
<p>
<script type="math/tex; mode=display">
\hat{Q}_n(\theta) = \left(\frac{1}{n} \sum_{i=1}^n g(z_i,
\theta)\right)' \hat{W} \left(\frac{1}{n} \sum_{i=1}^n g(z_i,
\theta)\right)
</script>
</p>
<p>and nonlinear least squares</p>
<p>
<script type="math/tex; mode=display">
\hat{Q}_n(\theta) =
\frac{1}{n} \sum_{i=1}^n (y_i - h(x_i,\theta))^2.
</script>
</p>
<p>See Newey and McFadden (1994)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> for more details and examples.</p>
<h2 id="example-logit">Example: logit<a class="headerlink" href="#example-logit" title="Permanent link">&para;</a></h2>
<p>As a simple example, let&rsquo;s look look at some code for estimating a
logit.</p>
<pre><code class="language-julia">using Distributions, Optim, BenchmarkTools
import ForwardDiff
function simulate_logit(observations, β)
  x = randn(observations, length(β))
  y = (x*β + rand(Logistic(), observations)) .&gt;= 0.0
  return((y=y,x=x))
end

function logit_likelihood(β,y,x)
  p = map(xb -&gt; cdf(Logistic(),xb), x*β)
  sum(log.(ifelse.(y, p, 1.0 .- p)))
end

n = 500
k = 3
β0 = ones(k)
(y,x) = simulate_logit(n,β0)
Q = β -&gt; -logit_likelihood(β,y,x)
Q(β0)
</code></pre>
<pre><code>238.08693921800221
</code></pre>
<p>Now we maximize the likelihood using a few different algorithms from <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a></p>
<pre><code class="language-julia">@time optimize(Q, zeros(k), NelderMead())
@time optimize(Q, zeros(k), BFGS(), autodiff = :forward)
@time optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward)
</code></pre>
<pre><code>0.002197 seconds (1.26 k allocations: 2.086 MiB)
  0.733200 seconds (2.60 M allocations: 136.030 MiB, 99.83% compilation tim
e)
  1.273711 seconds (4.01 M allocations: 207.743 MiB, 3.92% gc time, 99.88% 
compilation time)
 * Status: success

 * Candidate solution
    Final objective value:     2.370103e+02

 * Found with
    Algorithm:     Newton's Method (Trust Region)

 * Convergence measures
    |x - x'|               = 4.95e-08 ≰ 0.0e+00
    |x - x'|/|x'|          = 4.13e-08 ≰ 0.0e+00
    |f(x) - f(x')|         = 1.42e-13 ≰ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 6.00e-16 ≰ 0.0e+00
    |g(x)|                 = 6.75e-14 ≤ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    6
    f(x) calls:    7
    ∇f(x) calls:   7
    ∇²f(x) calls:  7
</code></pre>
<h3 id="aside-reverse-mode-automatic-differentiation">Aside: Reverse mode automatic differentiation<a class="headerlink" href="#aside-reverse-mode-automatic-differentiation" title="Permanent link">&para;</a></h3>
<p>For functions $f:\R^n \to \R^m$, the work for forward automatic
differentiation increases linearly with $n$. This is because forward
automatic differentiation applies the chain rule to each of the $n$
inputs. An alternative, is reverse automatic differentiation. Reverse
automatic differentiation is also based on the chain rule, but it
works backward from $f$ through intermediate steps back to $x$. The
work needed here scales linearly with $m$. Since optimization problems
have $m=1$, reverse automatic differentiation can often work well. The
downsides of reverse automatic differentiation are that: (1) it can
require a large amount of memory and (2) it is more difficult to
implement. There are handful of Julia packages that provide reverse
automatic differentiation, but they have some limitations in terms of
what functions thay can differentiate. Flux.jl and Zygote.jl are two such packages.</p>
<pre><code class="language-julia">using Optim, BenchmarkTools
import Zygote
dQr = β-&gt;Zygote.gradient(Q,β)[1]
dQf = β-&gt;ForwardDiff.gradient(Q,β)

@show dQr(β0) ≈ dQf(β0)

@btime dQf(β0)
@btime dQr(β0)

n = 500
k = 200
β0 = ones(k)
(y,x) = simulate_logit(n,β0)
Q = β -&gt; -logit_likelihood(β,y,x)
dQr = β-&gt;Zygote.gradient(Q,β)[1]
dQf = β-&gt;ForwardDiff.gradient(Q,β)
@show dQr(β0) ≈dQf(β0)
@btime dQf(β0);
@btime dQr(β0);
</code></pre>
<pre><code>dQr(β0) ≈ dQf(β0) = true
  23.387 μs (9 allocations: 47.77 KiB)
  66.415 μs (88 allocations: 143.73 KiB)
dQr(β0) ≈ dQf(β0) = true
  6.389 ms (141 allocations: 2.56 MiB)
  198.482 μs (89 allocations: 914.84 KiB)
</code></pre>
<h1 id="review-of-extremum-estimator-theory">Review of extremum estimator theory<a class="headerlink" href="#review-of-extremum-estimator-theory" title="Permanent link">&para;</a></h1>
<p>This is based on Newey and McFadden (1994)<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup><sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. You should already be familiar with this
from 627, so we will just state some basic &ldquo;high-level&rdquo; conditions for
consistency and asymptotic normality.</p>
<h2 id="consistency">Consistency<a class="headerlink" href="#consistency" title="Permanent link">&para;</a></h2>
<div class="admonition tip">
<p class="admonition-title"><strong>Theorem:</strong> consistency for extremum estimators</p>
<p>Assume</p>
<ol>
<li>
<p>$\hat{Q}_n(\theta)$ converges uniformly in probability to
$Q_0(\theta)$</p>
</li>
<li>
<p>$Q_0(\theta)$ is uniquely maximized at $\theta_0$.</p>
</li>
<li>
<p>$\Theta$ is compact and $Q_0(\theta)$ is continuous.</p>
</li>
</ol>
<p>Then $\hat{\theta} \inprob \theta_0$</p>
</div>
<h2 id="asymptotic-normality">Asymptotic normality<a class="headerlink" href="#asymptotic-normality" title="Permanent link">&para;</a></h2>
<div class="admonition tip">
<p class="admonition-title"><strong>Theorem:</strong> asymptotic normality for extremum estimators</p>
<p>Assume</p>
<ol>
<li>
<p>$\hat{\theta} \inprob \theta_0$</p>
</li>
<li>
<p>$\theta_0 \in interior(\Theta)$</p>
</li>
<li>
<p>$\hat{Q}<em N="N" _in="\in" _theta="\theta">n(\theta)$ is twice continuously differentiable in
open $N$ containing $\theta$ , and
$\sup</em> \Vert \nabla^2 \hat{Q}_n(\theta) - H(\theta) \Vert \inprob 0$
with $H(\theta_0)$ nonsingular</p>
</li>
<li>
<p>$\sqrt{n} \nabla \hat{Q}_n(\theta_0) \indist N(0,\Sigma)$</p>
</li>
</ol>
<p>Then $\sqrt{n} (\hat{\theta} - \theta_0) \indist N\left(0,H^{-1} \Sigma H^{-1} \right)$</p>
</div>
<p>Implementing this in Julia using automatic differentiation is straightforward.</p>
<pre><code class="language-julia">function logit_likei(β,y,x)
  p = map(xb -&gt; cdf(Logistic(),xb), x*β)
  log.(ifelse.(y, p, 1.0 .- p))
end

function logit_likelihood(β,y,x)
  mean(logit_likei(β,y,x))
end

n = 1000
k = 3
β0 = ones(k)
(y,x) = simulate_logit(n,β0)

Q = β -&gt; -logit_likelihood(β,y,x)
optres = optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward)
βhat = optres.minimizer

function asymptotic_variance(Q,dQi, θ)
  gi = dQi(θ)
  Σ = gi'*gi/size(gi)[1]
  H = ForwardDiff.hessian(Q,θ)
  invH = inv(H)
  (variance=invH*Σ*invH, Σ=Σ, invH=invH)
end

avar=asymptotic_variance(θ-&gt;logit_likelihood(θ,y,x),
                         θ-&gt;ForwardDiff.jacobian(β-&gt;logit_likei(β,y,x),θ),βhat)
display( avar.variance/n)
display( -avar.invH/n)
display(inv(avar.Σ)/n)
</code></pre>
<pre><code>3×3 Matrix{Float64}:
 0.00818827  0.00193123  0.00164696
 0.00193123  0.00794838  0.00192441
 0.00164696  0.00192441  0.00845101
3×3 Matrix{Float64}:
 0.00771534  0.00224534  0.00198786
 0.00224534  0.0079883   0.00210081
 0.00198786  0.00210081  0.00791974
3×3 Matrix{Float64}:
 0.00730949  0.00254474  0.00228685
 0.00254474  0.00804228  0.0022803
 0.00228685  0.0022803   0.00745307
</code></pre>
<p>For maximum likelihood, the information equality says $-H = \Sigma$,
so the three expressions above have the same probability limit, and
are each consistent estimates of the variance of $\hat{\theta}$.</p>
<p>The code above is for demonstration and learning. If we really wanted
to estimate a logit for research, it would be better to use a
well-tested package. Here&rsquo;s how to estimate  a logit using GLM.jl.</p>
<pre><code class="language-julia">using GLM, DataFrames
df = DataFrame(x, :auto)
df[!,:y] = y
glmest=glm(@formula(y ~ -1 + x1+x2+x3), df, Binomial(),LogitLink())
display( glmest)
display( vcov(glmest))
</code></pre>
<pre><code>StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{
Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, Cholesk
y{Float64, Matrix{Float64}}}}, Matrix{Float64}}

y ~ 0 + x1 + x2 + x3

Coefficients:
───────────────────────────────────────────────────────────────
       Coef.  Std. Error      z  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────
x1  0.918876   0.0878288  10.46    &lt;1e-24   0.746735    1.09102
x2  0.976711   0.0893682  10.93    &lt;1e-27   0.801553    1.15187
x3  0.949527   0.0889833  10.67    &lt;1e-25   0.775123    1.12393
───────────────────────────────────────────────────────────────
3×3 Matrix{Float64}:
 0.00771389  0.0022444   0.00198692
 0.0022444   0.00798667  0.00209977
 0.00198692  0.00209977  0.00791803
</code></pre>
<h2 id="delta-method">Delta method<a class="headerlink" href="#delta-method" title="Permanent link">&para;</a></h2>
<p>In many models, we are interested in some transformation of the
parameters in addition to the parameters themselves. For example, in a
logit, we might want to report marginal effects in addition to the
coefficients. In structural models, we typically use the parameter
estimates to conduct counterfactual simulations. In many
situations we are more interested these transformation(s) of
parameters than in the parameters themselves. The delta method is one
convenient way to approximate the distribution of transformations of
the model parameters.</p>
<div class="admonition tip">
<p class="admonition-title"><strong>Theorem:</strong> Delta method</p>
<p>Assume:</p>
<ol>
<li>
<p>$\sqrt{n} (\hat{\theta} - \theta_0) \indist N(0,\Omega)$</p>
</li>
<li>
<p>$g: \R^k \to \R^m$ is continuously differentiable</p>
</li>
</ol>
<p>Then $\sqrt{n}(g(\hat{\theta}) - g(\theta_0)) \indist N(0, \nabla g(\theta_0)^T \Omega \nabla g(\theta_0)$</p>
</div>
<p>The following code uses the delta method to plot a 90% pointwise
confidence band around the estimate marginal effect of one of the
regressors.</p>
<pre><code class="language-julia">using LinearAlgebra
function logit_mfx(β,x)
  ForwardDiff.jacobian(x-&gt; map(xb -&gt; cdf(Logistic(),xb), x*β), x)
end

function delta_method(g, θ, Ω)
  dG = ForwardDiff.jacobian(θ-&gt;g(θ),θ)
  dG*Ω*dG'
end

nfx = 100
xmfx = zeros(nfx,3)
xmfx[:,1] .= -3.0:(6.0/(nfx-1)):3.0

mfx = logit_mfx(βhat,xmfx)
vmfx = delta_method(β-&gt;logit_mfx(β,xmfx)[:,1], βhat, avar.variance/n)
sdfx = sqrt.(diag(vmfx))

using Plots, LaTeXStrings
Plots.gr()
#Plots.unicodeplots()
plot(xmfx[:,1],mfx[:,1],ribbon=quantile(Normal(),0.95)*sdfx,fillalpha=0.5,xlabel=L&quot;x_1&quot;, ylabel=L&quot;\frac{\partial}{\partial x_1}P(y=1|x)&quot;, legend=false,title=&quot;Marginal effect of x[1] when x[2:k]=0&quot;)
</code></pre>
<p><img alt="" src="../figures/extremumEstimation_7_1.png" /></p>
<p>The same approach can be used to compute standard errors and
confidence regions for the results of more complicated counterfactual
simulations, as long as the associated simulations are smooth
functions of the parameters. However, sometimes it might be more
natural to write simulations with outcomes that are not smooth in the
parameters. For example, the following code uses simulation to
calculate the change in the probability of $y$ from adding 0.1 to
$x$.</p>
<pre><code class="language-julia">function counterfactual_sim(β, x, S)
  function onesim()
    e = rand(Logistic(), size(x)[1])
    baseline= (x*β .+ e .&gt; 0)
    counterfactual= ((x.+0.1)*β .+ e .&gt; 0)
    mean(counterfactual.-baseline)
  end
  mean([onesim() for s in 1:S])
end
ForwardDiff.gradient(β-&gt;counterfactual_sim(β,x,10),βhat)
</code></pre>
<pre><code>3-element Vector{Float64}:
 0.0
 0.0
 0.0
</code></pre>
<p>Here, the gradient is 0 because the simulation function is a
step-function. In this situation, an alternative to the delta method
is the simulation based approach of Krinsky and Robb (1986)<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. The procedure is
quite simple. Suppose
$\sqrt{n}(\hat{\theta} - \theta_0) \indist N(0,\Omega)$,
and you want to an estimate of the distribution of $g(\theta)$.
Repeatedly draw $\theta_s \sim N(\hat{\theta}, \Omega/n)$ and compute
$g(\theta_s)$. Use the distribution of $g(\theta_s)$ for
inference. For example, a 90% confidence interval for $g(\theta)$
would be the 5%-tile of $g(\theta_s)$ to the 95%-tile of
$g(\theta_s)$.</p>
<pre><code class="language-julia">Ω = avar.variance/n
Ω = (Ω+Ω')/2         # otherwise, it's not exactly symmetric due to
                     # floating point roundoff
function kr_confint(g, θ, Ω, simulations; coverage=0.9)
  θs = [g(rand(MultivariateNormal(θ,Ω))) for s in 1:simulations]
  quantile(θs, [(1.0-coverage)/2, coverage + (1.0-coverage)/2])
end

@show kr_confint(β-&gt;counterfactual_sim(β,x,10), βhat, Ω, 1000)

# a delta method based confidence interval for the same thing
function counterfactual_calc(β, x)
  baseline      = cdf.(Logistic(), x*β)
  counterfactual= cdf.(Logistic(), (x.+0.1)*β)
  return([mean(counterfactual.-baseline)])
end
v = delta_method(β-&gt;counterfactual_calc(β,x), βhat, Ω)
ghat = counterfactual_calc(βhat,x)
@show [ghat + sqrt(v)*quantile(Normal(),0.05), ghat +
       sqrt(v)*quantile(Normal(),0.95)]
</code></pre>
<pre><code>kr_confint((β-&gt;begin
            #= /home/paul/.julia/dev/GMMInference/docs/jmd/extremumEstimati
on.jmd:10 =#
            counterfactual_sim(β, x, 10)
        end), βhat, Ω, 1000) = [0.0432, 0.052305]
[ghat + sqrt(v) * quantile(Normal(), 0.05), ghat + sqrt(v) * quantile(Norma
l(), 0.95)] = [[0.045645901625267424;;], [0.05070264808429101;;]]
2-element Vector{Matrix{Float64}}:
 [0.045645901625267424;;]
 [0.05070264808429101;;]
</code></pre>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Whitney K. Newey and Daniel McFadden. Chapter 36 large sample estimation and hypothesis testing. In <em>Handbook of Econometrics</em>, volume 4 of Handbook of Econometrics, pages 2111 &ndash; 2245. Elsevier, 1994. URL: <a href="http://www.sciencedirect.com/science/article/pii/S1573441205800054">http://www.sciencedirect.com/science/article/pii/S1573441205800054</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4">doi:https://doi.org/10.1016/S1573-4412(05)80005-4</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Whitney K. Newey and Daniel McFadden. Chapter 36 large sample estimation and hypothesis testing. In <em>Handbook of Econometrics</em>, volume 4 of Handbook of Econometrics, pages 2111 &ndash; 2245. Elsevier, 1994. URL: <a href="http://www.sciencedirect.com/science/article/pii/S1573441205800054">http://www.sciencedirect.com/science/article/pii/S1573441205800054</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4">doi:https://doi.org/10.1016/S1573-4412(05)80005-4</a>.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Itzhak Krinsky and A. Leslie Robb. On approximating the statistical properties of elasticities. <em>The Review of Economics and Statistics</em>, 68(4):715–719, 1986. URL: <a href="http://www.jstor.org/stable/1924536">http://www.jstor.org/stable/1924536</a>.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>

        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
