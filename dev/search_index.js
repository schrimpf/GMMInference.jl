var documenterSearchIndex = {"docs":
[{"location":"empiricalLikelihood/#Empirical-likelihood","page":"Empirical Likelihood","title":"Empirical likelihood","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"An interesting alternative to GMM is (generalized) empirical likelihood (GEL). Empirical likelihood has some appealing higher-order statistical properties. In particular, it can be shown to have lower higher order asymptotic bias than GMM. See (Newey and Smith, 2004). Relatedly, certain test statistics based on EL are robust to weak identification (Guggenberger and Smith, 2005). In fact, the identification robust tests that we have discusses are all based on the CUE-GMM objective function. The CUE-GMM objetive is a special case of generalized empirical likelihood.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"A perceived downside of GEL is that it involves a more difficult looking optimization problem than GMM. However, given the ease with which Julia can solve high dimensional optimization problems, GEL is very feasible. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"As in the extremum estimation notes, suppose we have moment conditions such that","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"mathrmEg_i(theta) = 0","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"where g_iR^d to R^k are some data dependent moment conditions. The empirical likelihood estimator solves","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"beginalign*\n    (hattheta hatp) =  argmax_thetap frac1n sum_i\n    log(p_i)  st  \n      sum_i p_i = 1  0leq p_i leq 1 \n      sum_i p_i g_i(theta) = 0 \nendalign*","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Generalized empirical likelihood replaces log(p) with some other convex function h(p), ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"beginalign*\n    (hattheta^GELh hatp) =  argmin_thetap\n    frac1nsum_i h(p_i)  st  \n      sum_i p_i = 1  0leq p leq 1 \n      sum_i p_i g_i(theta) = 0 \nendalign*","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"setting h(p) = frac12(p^2-(1n)^2) results in an estimator identical to the CUE-GMM estimator.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"A common approach to computing GEL estimators is to eliminate pi by looking at the dual problem","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"hattheta^GEL  = argmin_thetasup_lambda sum_i rho(lambdag_i(theta))","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"where rho is some function related to h. See (Newey and Smith, 2004) for details. There can be some analytic advantages to doing so, but computationally, the original statement of the problem has some advantages. First, there is more existing software for solving constrained minimization problems than for solving saddle point problems. Second, although p is high dimensional, it enters the constraints linearly, and the objective function is concave. Many optimization algorithms will take good advantage of this. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Let's look at some Julia code. Since the problem involves many variables with linear constraints, it is worthwhile to use JuMP for optimization. The code is slightly more verbose, but the speed of JuMP (and the Ipopt solver) are often worth it.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"using GMMInference, JuMP, Ipopt, LinearAlgebra, Distributions \nimport Random\n\nn = 300\nd = 4\nk = 2*d\nβ0 = ones(d)\nπ0 = vcat(I,ones(k-d,d))\nρ = 0.5\nRandom.seed!(622)\ndata = IVLogitShare(n, β0, π0, ρ);","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"# set up JuMP problem\nTy = quantile.(Logistic(),data.y)   \nm = Model()\n@variable(m, 0.0 <= p[1:n] <= 1.0)\n@variable(m, θ[1:d])\n@constraint(m, prob,sum(p)==1.0)\n@constraint(m, momentcon[i=1:k], dot((Ty - data.x*θ).*data.z[:,i],p)==0.0)\n@NLobjective(m,Max, sum(log(p[i]) for i in 1:n))\nm","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"The gel_jump_problem function from GMMInference.jl does the same thing as the above code cell. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Let's solve the optimization problem.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"set_optimizer(m, optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" => 5))\nset_start_value.(m[:θ], 0.0)\nset_start_value.(m[:p], 1/n)\noptimize!(m)\n@show value.(m[:θ])\n@show value.(m[:p][1:10])","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"For comparison here is how long it takes JuMP + Ipopt to solve for the CUE-GMM estimator. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"@show mcue = gmm_jump_problem(data, cue_objective)\nset_start_value.(mcue[:θ], 0.0)\nset_optimizer(mcue,  optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =>5))\noptimize!(mcue)\n@show value.(mcue[:θ]) ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"In this comparison, EL is both faster and more robust to initial values than CUE-GMM. GMM with a fixed weighting matrix will likely be faster than either.","category":"page"},{"location":"empiricalLikelihood/#GEL-with-other-optimization-packages","page":"Empirical Likelihood","title":"GEL with other optimization packages","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"We can also estimate GEL models with other optimization packages. Relative to JuMP, other optimization packages have the advantage that the problem does not have to be written in any special syntax. However, other packages have the downside that they will not recognize any special structure in the constraints (linear, quadratic, sparse, etc) unless we explicitly provide it. Let's see how much, if any difference this makes to performance. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Here we will use the NLPModels.jl interface to Ipopt. Essentially, all this does is call ForwardDiff on the objective function and constraints, and then give the resulting gradient and hessian functions to Ipopt.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"using NLPModelsIpopt\ngel = gel_nlp_problem(data)\nip = ipopt(gel)","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"As you see, this approach is far slower than with JuMP. Notice that the number of iterations and function evaluations are identical. The big difference is that JuMP evaluates the function (and its derivatives) very quickly, while NLP takes much much longer. I would guess that this is largely because it is using ForwardDiff to calculate a gradients and hessians for 304 variables.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Let's also estimate the model using Optim.jl. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"using Optim\nargs = gel_optim_args(data)\n@time opt=optimize(args[1],args[2],\n                   [fill(1/(1.1*n),n)..., zeros(d)...],\n                   IPNewton(μ0=:auto, show_linesearch=false),\n                   Optim.Options(show_trace=true,\n                                 allow_f_increases=true,\n                                 successive_f_tol=10,\n                                 allow_outer_f_increases=true))\n@show opt.minimizer[(n+1):end]","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"The IPNewton() optimizer from Optim.jl appears to be much less efficient than Ipopt. IPNewton takes more than 4 times as many iterations. ","category":"page"},{"location":"empiricalLikelihood/#Inference-for-EL","page":"Empirical Likelihood","title":"Inference for EL","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"(Guggenberger and Smith, 2005) show that GEL versions of the AR and LM statistics are robust to weak identification. The GEL version of the AR statistic is the generalized empirical likelihood ratio. Specifically, (Guggenberger and Smith, 2005) show that","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"GELR(theta_0) = 2sum_i=1^nleft(h(p_i(theta_0))  -\n   h(1n)right) leadsto chi^2_k ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"where p_i(theta_0) are given by","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"beginalign*\n    p(theta)  =  argmax_0 leq p leq 1 sum h(p_i) text st   sum_i p_i\n    = 1 \n sum_i p_i g_i(theta) = 0\nendalign*","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"The GELR statistic shares the downsides of the AR statistic –- the degrees of freedom is the number of moments instead of the number of parameters, which tends to lead to lower power in overidentified models; and it combines a test of misspecification with a location test for theta. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Consequently, it can be useful to instead look at a Lagrange multiplier style statistic. The true theta maximizes the empirical likelihood, so ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"0 = sum_i=1^n nabla_theta h(p_i(theta_0)) = lambda(theta_0) sum_i=1^n\np_i(theta_0) nabla_theta g_i(theta_0) equiv lambda(theta_0) D(theta_0)","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"where p_i(theta_0) is as defined above, and lambda(theta_0) are the mulitpliers on the empirical moment condition constraint. Andrews and Guggenberger show that a quadratic form in the above score equation is asymptotically chi^2_d. To be specific, let Delta(theta) = E(1nsum_i g_i(theta) - Eg(theta))(1n sum_i  g_i(theta) - Eg(theta))  and define","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"S(theta) = nlambda(theta) D(theta) left( D(theta)\nDelta(theta)^-1 D(theta) right)^-1 D(theta)lambda(theta)","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"then S(theta_0) leadsto chi^2_d. This result holds whether or not theta is strongly identified. ","category":"page"},{"location":"empiricalLikelihood/#Implementation","page":"Empirical Likelihood","title":"Implementation","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Computing the GELR and S statistics requires solving a linear program for each theta we want to test. Fortunately, linear programs can be solved very quickly. See gel_pλ in GMMInference.jl for the relevant code. ","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"Let's do a simulation to check that these tests have correct coverage.","category":"page"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"using Plots\nPlots.gr()\nS = 500\nn = 200\nd = 2\nβ0 = ones(d)\nρ = 0.5\nk = 3\nfunction sim_p(π0)\n  data = IVLogitShare(n, β0, π0, ρ)\n  GELR, S, plr, ps = gel_tests(β0, data)\n  [plr ps]\nend\nπweak = ones(k,d) .+ vcat(diagm(0=>fill(0.001,d)),zeros(k-d,d))  \nπstrong = vcat(5*diagm(0=>ones(d)),ones(k-d,d)) \npweak=vcat([sim_p(πweak ) for s in 1:S]...)\npstrong=vcat([sim_p(πstrong) for s in 1:S]...)\n\npgrid = 0:0.01:1\nplot(pgrid, p->(mean( pstrong[:,1] .<= p)-p), legend=:bottomleft,\n     label=\"GELR, strong ID\", style=:dash, color=:blue,\n     xlabel=\"p\", ylabel=\"P(p value < p)-p\",\n     title=\"Simulated CDF of p-values - p\")  \nplot!(pgrid, p->(mean( pstrong[:,2] .<= p)-p),\n      label=\"S, strong ID\", style=:dash, color=:green)\n\nplot!(pgrid, p->(mean( pweak[:,1] .<= p)-p),\n      label=\"GELR, weak ID\", style=:solid, color=:blue)\nplot!(pgrid, p->(mean( pweak[:,2] .<= p)-p),\n      label=\"S, weak ID\", style=:solid, color=:green)\nplot!(pgrid,0,alpha=0.5, label=\"\")","category":"page"},{"location":"empiricalLikelihood/#Subvector-inference","page":"Empirical Likelihood","title":"Subvector inference","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"(Guggenberger and Smith, 2005) also give results for subvector inference. Let (alpha beta) =theta. Assume beta is strongly identified. Guggenberger and Smith show that analogs of GELR and S with beta concentrated out lead to valid tests for alpha, whether alpha is weakly or strongly identified. ","category":"page"},{"location":"empiricalLikelihood/#Bootstrap-for-EL","page":"Empirical Likelihood","title":"Bootstrap for EL","text":"","category":"section"},{"location":"empiricalLikelihood/","page":"Empirical Likelihood","title":"Empirical Likelihood","text":"For bootstrapping GMM, we discussed how it is important that the null hypothesis holds in the bootstrapped data. In GMM we did this by substracting the sample averages of the moments. In GEL, an alternative way to impose the null, is to sample the data with probabilities hatp_i instead of with equal proability. See (Brown and Newey, 2002) for more information. ","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"<div class=\"citation canonical\"><ul><li>\n<div id=\"abadie2008\">Abadie, A. and Imbens, G. W. (2008). <a href='https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474'><i>On the Failure of the Bootstrap for Matching Estimators</i></a>. <a href='https://doi.org/10.3982/ECTA6474'>Econometrica <b>76</b>, 1537-1557</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6474'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6474</a>.</div>\n</li><li>\n<div id=\"andrews2015\">Andrews, D. and Guggenberger, P. (2015). <a href='https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2545374'><i>Identification-and singularity-robust inference for moment condition models</i></a>.</div>\n</li><li>\n<div id=\"andrews2000\">Andrews, D. W. (2000). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00114'><i>Inconsistency of the Bootstrap when a Parameter is on the Boundary of the Parameter Space</i></a>. <a href='https://doi.org/10.1111/1468-0262.00114'>Econometrica <b>68</b>, 399-405</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00114'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00114</a>.</div>\n</li><li>\n<div id=\"andrews2009\">Andrews, D. W. and Han, S. (2009). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1368-423X.2008.00265.x'><i>Invalidity of the bootstrap and the m out of n bootstrap for confidence interval endpoints defined by moment inequalities</i></a>. <a href='https://doi.org/10.1111/j.1368-423X.2008.00265.x'>The Econometrics Journal <b>12</b>, S172-S199</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1368-423X.2008.00265.x'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1368-423X.2008.00265.x</a>.</div>\n</li><li>\n<div id=\"andrews2017\">Andrews, D. W. and Guggenberger, P. (2017). <i>Asymptotic size of\n                  Kleibergen’s LM and conditional LR tests for moment\n                  condition models</i>. <a href='https://doi.org/10.1017/S0266466616000347'>Econometric Theory <b>33</b>, 1046–1080</a>.</div>\n</li><li>\n<div id=\"brown2002\">Brown, B. W. and Newey, W. K. (2002). <a href=' \n        https://doi.org/10.1198/073500102288618649\n    \n'><i>Generalized Method of Moments, Efficient Bootstrapping, and Improved Inference</i></a>. <a href='https://doi.org/10.1198/073500102288618649'>Journal of Business \\& Economic Statistics <b>20</b>, 507-517</a>, <a href='https://arxiv.org/abs/ \n        https://doi.org/10.1198/073500102288618649\n    \n'>arXiv: \n        https://doi.org/10.1198/073500102288618649\n    \n</a>.</div>\n</li><li>\n<div id=\"caner2009\">Caner, M. (2009). <a href=' \n        https://doi.org/10.1080/07474930903451599\n    \n'><i>Testing, Estimation in GMM and CUE with Nearly-Weak Identification</i></a>. <a href='https://doi.org/10.1080/07474930903451599'>Econometric Reviews <b>29</b>, 330-363</a>, <a href='https://arxiv.org/abs/ \n        https://doi.org/10.1080/07474930903451599\n    \n'>arXiv: \n        https://doi.org/10.1080/07474930903451599\n    \n</a>.</div>\n</li><li>\n<div id=\"chernozhukov2017\">Chernozhukov, V.; Chetverikov, D. and Kato, K. (2017). <a href='https://doi.org/10.1214/16-AOP1113'><i>Central limit theorems and bootstrap in high dimensions</i></a>. <a href='https://doi.org/10.1214/16-AOP1113'>Ann. Probab. <b>45</b>, 2309–2352</a>.</div>\n</li><li>\n<div id=\"gine1997\">Gine, E. (1997). <a href='https://doi.org/10.1007/BFb0092619'><i>Lectures on some aspects of the bootstrap</i></a>. <a href='https://doi.org/10.1007/BFb0092619'>Lectures on Probability Theory and Statistics: Ecole d'Et{\\'e} de Probabilit{\\'e}s de Saint-Flour XXVI-1996, 37–151, Springer Berlin Heidelberg, Berlin, Heidelberg</a>.</div>\n</li><li>\n<div id=\"guggenberger2005\">Guggenberger, P. and Smith, R. J. (2005). <i>Generalized empirical likelihood estimators and tests under partial, weak, and strong identification</i>. <a href='https://doi.org/10.1017/S0266466605050371'>Econometric Theory <b>21</b>, 667–709</a>.</div>\n</li><li>\n<div id=\"hall1994\">Hall, P. (1994). <a href='http://www.sciencedirect.com/science/article/pii/S157344120580008X'><i>Chapter 39 Methodology and theory for the bootstrap</i></a>. <a href='https://doi.org/https://doi.org/10.1016/S1573-4412(05)80008-X'>In: Handbook of Econometrics, editors, 2341 - 2381. Elsevier</a>.</div>\n</li><li>\n<div id=\"horowitz2001\">Horowitz, J. L. (2001). <a href='http://www.sciencedirect.com/science/article/pii/S157344120105005X'><i>Chapter 52 - The Bootstrap</i></a>. <a href='https://doi.org/https://doi.org/10.1016/S1573-4412(01)05005-X'>In: Handbook of Econometrics, editors, James J. Heckman and Edward Leamer, 3159 - 3228. Elsevier</a>.</div>\n</li><li>\n<div id=\"kleibergen2005\">Kleibergen, F. (2005). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00610.x'><i>Testing Parameters in GMM Without Assuming that They Are Identified</i></a>. <a href='https://doi.org/10.1111/j.1468-0262.2005.00610.x'>Econometrica <b>73</b>, 1103-1123</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2005.00610.x'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2005.00610.x</a>.</div>\n</li><li>\n<div id=\"krinsky1986\">Krinsky, I. and Robb, A. L. (1986). <a href='http://www.jstor.org/stable/1924536'><i>On Approximating the Statistical\n                  Properties of Elasticities</i></a>. The Review of Economics and\n                  Statistics <b>68</b>, 715–719.</div>\n</li><li>\n<div id=\"mackinnon2009\">MacKinnon, J. G. (2009). <a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1033.330&rep=rep1&type=pdf#page=203'><i>Bootstrap hypothesis testing</i></a>. Handbook of Computational Econometrics <b>183</b>, 213.</div>\n</li><li>\n<div id=\"mackinnon2006\">MacKinnon, J. G. (2006). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2006.00328.x'><i>Bootstrap Methods in Econometrics*</i></a>. <a href='https://doi.org/10.1111/j.1475-4932.2006.00328.x'>Economic Record <b>82</b>, S2-S18</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-4932.2006.00328.x'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-4932.2006.00328.x</a>.</div>\n</li><li>\n<div id=\"moreira2003\">Moreira, M. J. (2003). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00438'><i>A Conditional Likelihood Ratio Test for Structural Models</i></a>. <a href='https://doi.org/10.1111/1468-0262.00438'>Econometrica <b>71</b>, 1027-1048</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00438'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00438</a>.</div>\n</li><li>\n<div id=\"newey1994\">Newey, W. K. and McFadden, D. (1994). <a href='http://www.sciencedirect.com/science/article/pii/S1573441205800054'><i>Chapter 36 Large sample estimation and hypothesis testing</i></a>. <a href='https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4'>In: Handbook of Econometrics, editors, 2111 - 2245. Elsevier</a>.</div>\n</li><li>\n<div id=\"newey2004\">Newey, W. K. and Smith, R. J. (2004). <a href='https://www.jstor.org/stable/pdf/3598854.pdf?casa_token=m7RohRNDzqcAAAAA:rOXGanSJq8u2gpjAg1T-K_dJxhE_PoNwtFNQGV6YHwZ_0WlTnc_9jj5qbCeGpo6ekF1WGEcBPG9wLelRbrBziziFD6inZ_W_wRuza_UZKJzJSYnwtw7vOA'><i>Higher order properties of GMM and generalized empirical likelihood estimators</i></a>. Econometrica <b>72</b>, 219–255.</div>\n</li><li>\n<div id=\"romano2012\">Romano, J. P. and Shaikh, A. M. (2012). <a href='https://doi.org/10.1214/12-AOS1051'><i>On the uniform asymptotic validity of subsampling and the bootstrap</i></a>. <a href='https://doi.org/10.1214/12-AOS1051'>Ann. Statist. <b>40</b>, 2798–2822</a>.</div>\n</li><li>\n<div id=\"shi2012\">Shi, X. (2012). <a href='https://www.ssc.wisc.edu/~xshi/econ715/Lecture_10_bootstrap.pdf'><i>Lecture 10: Bootstrap</i></a>.</div>\n</li><li>\n<div id=\"stock2002\">Stock, J. H.; Wright, J. H. and Yogo, M. (2002). <a href='\n        https://doi.org/10.1198/073500102288618658\n'><i>A Survey of Weak Instruments and Weak Identification in\n                  Generalized Method of Moments</i></a>. <a href='https://doi.org/10.1198/073500102288618658'>Journal of Business \\& Economic\n                  Statistics <b>20</b>, 518-529</a>, <a href='https://arxiv.org/abs/\n        https://doi.org/10.1198/073500102288618658\n'>arXiv:\n        https://doi.org/10.1198/073500102288618658\n</a>.</div>\n</li><li>\n<div id=\"stock2000\">Stock, J. H. and Wright, J. H. (2000). <a href='https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00151'><i>GMM with Weak Identification</i></a>. <a href='https://doi.org/10.1111/1468-0262.00151'>Econometrica <b>68</b>, 1055-1096</a>, <a href='https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00151'>arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00151</a>.</div>\n</li><li>\n<div id=\"van2000\">Van der Vaart, A. W. (2000). <i>Asymptotic statistics</i>. Cambridge university press.</div>\n</li>\n</ul></div>","category":"page"},{"location":"license/","page":"License","title":"License","text":"The notes and examples are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.","category":"page"},{"location":"license/","page":"License","title":"License","text":"(Image: )","category":"page"},{"location":"license/","page":"License","title":"License","text":"BibTeX citation.","category":"page"},{"location":"license/","page":"License","title":"License","text":"The license for the package source code is here.","category":"page"},{"location":"extremumEstimation/#Extremum-Estimation","page":"Extremum Estimation","title":"Extremum Estimation","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"defindepperpperp\ndefErmathrmE\ndefRmathbbR\ndefEnmathbbE_n\ndefPrmathrmP\nDeclareMathOperator*argmaxargmax\nDeclareMathOperator*argminargmin\ndefinprobbuildrel p over rightarrow\ndefindistbuildrel d over rightarrow\n","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"Many, perhaps most, estimators in econometrics are extrumem estimators. That is, many estimators are defined by","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"hattheta = argmax_theta in Theta\nhatQ_n(theta)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"where hatQ_n(theta) is some objective function that depends on data. Examples include maximum likelihood,","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"hatQ_n(theta) = frac1n sum_i=1^n f(z_i  theta)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"GMM,","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"hatQ_n(theta) = left(frac1n sum_i=1^n g(z_i\ntheta)right) hatW left(frac1n sum_i=1^n g(z_i\ntheta)right)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"and nonlinear least squares","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"hatQ_n(theta) =\nfrac1n sum_i=1^n (y_i - h(x_itheta))^2","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"See (Newey and McFadden, 1994) for more details and examples.","category":"page"},{"location":"extremumEstimation/#Example:-logit","page":"Extremum Estimation","title":"Example: logit","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"As a simple example, let's look look at some code for estimating a logit.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"using Distributions, Optim, BenchmarkTools\nimport ForwardDiff\nfunction simulate_logit(observations, β)\n  x = randn(observations, length(β))\n  y = (x*β + rand(Logistic(), observations)) .>= 0.0\n  return((y=y,x=x))\nend\n\nfunction logit_likelihood(β,y,x)\n  p = map(xb -> cdf(Logistic(),xb), x*β)\n  sum(log.(ifelse.(y, p, 1.0 .- p)))\nend\n\nn = 500\nk = 3\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\nQ = β -> -logit_likelihood(β,y,x)\nQ(β0)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"Now we maximize the likelihood using a few different algorithms from Optim.jl","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"@btime nm=optimize(Q, zeros(k), NelderMead())\n@btime bfgs=optimize(Q, zeros(k), BFGS(), autodiff = :forward)\n@btime ntr=optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward);","category":"page"},{"location":"extremumEstimation/#Aside:-Reverse-mode-automatic-differentiation","page":"Extremum Estimation","title":"Aside: Reverse mode automatic differentiation","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"For functions fR^n to R^m, the work for forward automatic differentiation increases linearly with n. This is because forward automatic differentiation applies the chain rule to each of the n inputs. An alternative, is reverse automatic differentiation. Reverse automatic differentiation is also based on the chain rule, but it works backward from f through intermediate steps back to x. The work needed here scales linearly with m. Since optimization problems have m=1, reverse automatic differentiation can often work well. The downsides of reverse automatic differentiation are that: (1) it can require a large amount of memory and (2) it is more difficult to implement. There are handful of Julia packages that provide reverse automatic differentiation, but they have some limitations in terms of what functions thay can differentiate. Flux.jl and Zygote.jl are two such packages.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"using Optim, BenchmarkTools\nimport Zygote\ndQr = β->Zygote.gradient(Q,β)[1]\ndQf = β->ForwardDiff.gradient(Q,β)\n\n@show dQr(β0) ≈ dQf(β0)\n\n@btime dQf(β0)\n@btime dQr(β0)\n\nn = 500\nk = 200\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\nQ = β -> -logit_likelihood(β,y,x)\ndQr = β->Zygote.gradient(Q,β)[1]\ndQf = β->ForwardDiff.gradient(Q,β)\n@show dQr(β0) ≈dQf(β0)\n@btime dQf(β0);\n@btime dQr(β0);","category":"page"},{"location":"extremumEstimation/#Review-of-extremum-estimator-theory","page":"Extremum Estimation","title":"Review of extremum estimator theory","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"This is based on (Newey and McFadden, 1994). You should already be familiar with this from 627, so we will just state some basic \"high-level\" conditions for consistency and asymptotic normality.","category":"page"},{"location":"extremumEstimation/#Consistency","page":"Extremum Estimation","title":"Consistency","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"tip: Theorem: consistency for extremum estimators\nAssumehatQ_n(theta) converges uniformly in probability toQ_0(theta)Q_0(theta) is uniquely maximized at theta_0.\nTheta is compact and Q_0(theta) is continuous.Then hattheta to^p theta_0","category":"page"},{"location":"extremumEstimation/#Asymptotic-normality","page":"Extremum Estimation","title":"Asymptotic normality","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"tip: Theorem: asymptotic normality for extremum estimators\nAssumehattheta to^p theta_0\ntheta_0 in interior(Theta)\nhatQ_n(theta) is twice continuously differentiable inopen N containing theta , and sup_theta in N leftVert nabla^2 hatQ_n(theta) - H(theta) rightVert to^p 0 with H(theta_0) nonsingularsqrtn nabla hatQ_n(theta_0) leadsto N(0Sigma)Then sqrtn (hattheta - theta_0) leadsto Nleft(0H^-1 Sigma H^-1 right)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"Implementing this in Julia using automatic differentiation is straightforward.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"function logit_likei(β,y,x)\n  p = map(xb -> cdf(Logistic(),xb), x*β)\n  log.(ifelse.(y, p, 1.0 .- p))\nend\n\nfunction logit_likelihood(β,y,x)\n  mean(logit_likei(β,y,x))\nend\n\nn = 100\nk = 3\nβ0 = ones(k)\n(y,x) = simulate_logit(n,β0)\n\nQ = β -> -logit_likelihood(β,y,x)\noptres = optimize(Q, zeros(k), NewtonTrustRegion(), autodiff =:forward)\nβhat = optres.minimizer\n\nfunction asymptotic_variance(Q,dQi, θ)\n  gi = dQi(θ)\n  Σ = gi'*gi/size(gi)[1]\n  H = ForwardDiff.hessian(Q,θ)\n  invH = inv(H)\n  (variance=invH*Σ*invH, Σ=Σ, invH=invH)\nend\n\navar=asymptotic_variance(θ->logit_likelihood(θ,y,x),\n                         θ->ForwardDiff.jacobian(β->logit_likei(β,y,x),θ),βhat)\n@show avar.variance/n\n@show -avar.invH/n\n@show inv(avar.Σ)/n","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"For maximum likelihood, the information equality says -H = Sigma, so the three expressions above have the same probability limit, and are each consistent estimates of the variance of hattheta.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"The code above is for demonstration and learning. If we really wanted to estimate a logit for research, it would be better to use a well-tested package. Here's how to estimate  a logit using GLM.jl.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"using GLM, DataFrames\ndf = DataFrame(x, :auto)\ndf[!,:y] = y\nglmest=glm(@formula(y ~ -1 + x1+x2+x3), df, Binomial(),LogitLink())\n@show glmest\n@show vcov(glmest)","category":"page"},{"location":"extremumEstimation/#Delta-method","page":"Extremum Estimation","title":"Delta method","text":"","category":"section"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"In many models, we are interested in some transformation of the parameters in addition to the parameters themselves. For example, in a logit, we might want to report marginal effects in addition to the coefficients. In structural models, we typically use the parameter estimates to conduct counterfactual simulations. In many situations we are more interested these transformation(s) of parameters than in the parameters themselves. The delta method is one convenient way to approximate the distribution of transformations of the model parameters.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"tip: Theorem: Delta method\nAssume:sqrtn (hattheta - theta_0) leadsto N(0Omega)\ng R^k to R^m is continuously differentiableThen sqrtn(g(hattheta) - g(theta_0)) leadsto N(0 nabla g(theta_0)^T Omega nabla g(theta_0)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"The following code uses the delta method to plot a 90% pointwise confidence band around the estimate marginal effect of one of the regressors.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"using LinearAlgebra\nfunction logit_mfx(β,x)\n  out = ForwardDiff.jacobian(x-> map(xb -> cdf(Logistic(),xb), x*β), x)\n  out = reshape(out, size(out,1), size(x)...)\nend\n\nfunction delta_method(g, θ, Ω)\n  dG = ForwardDiff.jacobian(θ->g(θ),θ)\n  dG*Ω*dG'\nend\n\nnfx = 100\nxmfx = zeros(nfx,3)\nxmfx[:,1] .= -3.0:(6.0/(nfx-1)):3.0\n\nmfx = logit_mfx(βhat,xmfx)\nvmfx = delta_method(β->diag(logit_mfx(β,xmfx)[:,:,1]), βhat, avar.variance/n)\nsdfx = sqrt.(diag(vmfx))\n\nusing Plots, LaTeXStrings\nplot(xmfx[:,1],diag(mfx[:,:,1]),ribbon=quantile(Normal(),0.95)*sdfx,fillalpha=0.5,\n     xlabel=L\"x_1\", ylabel=L\"\\frac{\\partial}{\\partial x_1}P(y=1|x)\", \n     legend=false,title=\"Marginal effect of x[1] when x[2:k]=0\")","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"The same approach can be used to compute standard errors and confidence regions for the results of more complicated counterfactual simulations, as long as the associated simulations are smooth functions of the parameters. However, sometimes it might be more natural to write simulations with outcomes that are not smooth in the parameters. For example, the following code uses simulation to calculate the change in the probability of y from adding 0.1 to x.","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"function counterfactual_sim(β, x, S)\n  function onesim()\n    e = rand(Logistic(), size(x)[1])\n    baseline= (x*β .+ e .> 0)\n    counterfactual= ((x.+0.1)*β .+ e .> 0)\n    mean(counterfactual.-baseline)\n  end\n  mean([onesim() for s in 1:S])\nend\n@show ∇s = ForwardDiff.gradient(β->counterfactual_sim(β,x,10),βhat)","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"Here, the gradient is 0 because the simulation function is a step-function. In this situation, an alternative to the delta method is the simulation based approach of (Krinsky and Robb, 1986). The procedure is quite simple. Suppose sqrtn(hattheta - theta_0) leadsto N(0Omega), and you want to an estimate of the distribution of g(theta). Repeatedly draw theta_s sim N(hattheta Omegan) and compute g(theta_s). Use the distribution of g(theta_s) for inference. For example, a 90% confidence interval for g(theta) would be the 5%-tile of g(theta_s) to the 95%-tile of g(theta_s).","category":"page"},{"location":"extremumEstimation/","page":"Extremum Estimation","title":"Extremum Estimation","text":"Ω = avar.variance/n\nΩ = Symmetric((Ω+Ω')/2) # otherwise, it's not exactly symmetric due to\n                        # floating point roundoff\nfunction kr_confint(g, θ, Ω, simulations; coverage=0.9)\n  θs = [g(rand(MultivariateNormal(θ,Ω))) for s in 1:simulations]\n  quantile(θs, [(1.0-coverage)/2, coverage + (1.0-coverage)/2])\nend\n\n@show kr_confint(β->counterfactual_sim(β,x,10), βhat, Ω, 1000)\n\n# a delta method based confidence interval for the same thing\nfunction counterfactual_calc(β, x)\n  baseline      = cdf.(Logistic(), x*β)\n  counterfactual= cdf.(Logistic(), (x.+0.1)*β)\n  return([mean(counterfactual.-baseline)])\nend\nv = delta_method(β->counterfactual_calc(β,x), βhat, Ω)\nghat = counterfactual_calc(βhat,x)\n@show [ghat + sqrt(v)*quantile(Normal(),0.05), ghat +\n       sqrt(v)*quantile(Normal(),0.95)]","category":"page"},{"location":"bootstrap/#Bootstrap","page":"Bootstrap","title":"Bootstrap","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"The bootstrap is a method of inference that utilizes resampling. The basic idea is as follows. Suppose you have some parameter of interest for which you want to do inference. Let T_n denote some test statistic involving the estimator. The test  statistic is a function of data, so the distribution of the estimator is a function of the distribution of data. Let F_0 denote the exact, finite sample distribution of the data. Let ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"G_n(tau F_0) = Pr(hattheta_n leq tau)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"denote the exact finite sample distribution of the statistic.  To do inference, we would like to know G_n(tau F_0).  This is generally impossible without strong assumptions. Asymptotics get around this problem by approximating G_n(tau F_0) with its asymptotic distribution, G_infty(tauF_0). The bootstrap is an alternative approach (but the formal justification for the bootstrap still relies on asymptotics). The bootstrap approximates G_n(tau F_0) by replacing F_0 with an estimate, hatF_n. One common estimate of hatF_n is simply the empirical CDF. When observations are independent, we can randomly draw T^ast_n from  G_n(tau hatF_n) by randomly drawing with replacement a sample of size n from the orgininal observations, and then computing T^ast_n for this sample. We can do this repeatedly, and use the distribution of the resulting hattheta^ast_n's to calculate G_n(tauhatF_n). ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"As a quick example, here's some code where the statistic is the sample median minus its true value.","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"using Plots, StatsPlots, Distributions, Optim, ForwardDiff, LinearAlgebra, GMMInference\nimport Random\nPlots.gr();","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Random.seed!(622)\ndgp(n) = rand(n)\nestimator(x) = median(x)\n# simulating T ~ G_n(τ,F_0)\nn = 1000\nS = 999\nT = [estimator(dgp(n)) for s in 1:S] .- 0.5\nfunction bootstrap(data, estimator, S)\n  n = length(data)\n  θhat = estimator(data)\n  T = [estimator(sample(data,n, replace=true)) for s in 1:S] .- θhat \nend\nTboot = bootstrap(dgp(n),estimator, S)\ndensity(T, label=\"true distribution\")\ndensity!(Tboot, label=\"bootstrap distribution\")","category":"page"},{"location":"bootstrap/#References","page":"Bootstrap","title":"References","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"(MacKinnon, 2006) and (MacKinnon, 2009) are good practical introductions to the bootstrap. (Horowitz, 2001) is also a good overview, and includes more precise statements of theoretical results, but does not contain proofs. The lecture notes of (Shi, 2012) are another very good overview. (Gine, 1997) is a rigorous and fairly self-contained theoretical treatment of the bootstrap.","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Although the bootstrap works in many situations, it does not always work. For example, (Abadie and Imbens, 2008) show the failure of the bootstrap for matching estimators. See (Andrews, 2000),  (Andrews and Han, 2009), and (Romano and Shaikh, 2012) for theoretical developments on situations where the bootstrap fails and alternatives that work.   (Hall, 1994) gives a theoretical overview of when the bootstrap provides asymptotic refinement.  (Chernozhukov *et al.*, 2017) discusses the bootstrap in high dimensional models.","category":"page"},{"location":"bootstrap/#Theory","page":"Bootstrap","title":"Theory","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"This section follows the approach (Van der Vaart, 2000). We focus on the case where","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"T_n = frachattheta_n - theta_0hatsigma_n","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"is a t-statistic. A simple and useful result is that if T_n and T^ast_n both converge to the same distribution, then the bootstrap is consistent. ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"tip: Theroem: bootstrap consistency\nSuppose that T_n = frachattheta_n - theta_0hatsigma_n leadsto Tand T_n^ast =  frachattheta^ast_n - hattheta_nhatsigma^ast_n leadsto Tconditional on the data, for some random variable T with a continuous distribution function. Then  G_n(tau F_0) - G_n(tauhatF_n)  to^p 0 and in particular,Pr(theta_0 in hattheta_n - G_n^-1(alpha2 hatF_n)\nhatsigma_n   hattheta_n - G_n^-1(1-alpha2 hatF_n)\nhatsigma_n ) to 1-alpha","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"info: Proof skecth:\nT_n and T_n^ast both leadsto T immediately implies G_n(tau F_0) to^p G_infty(tau)andG_n(tauhatF_n) to^p G_infty(tau)where  G_infty(tau) is the CDF of  T . This implies that G^-1_n(tauhatF_n) to^p G^-1_infty(tau)for all tau where G_infty is continuous. Then we havePr(theta_0 geq hattheta_n - G_n^-1(tau hatF_n)\nhatsigma_n) = Prleft(fractheta_0 -\nhattheta_nhatsigma_n leq G_n^-1(tau hatF_n)right) to\nPr(T leq G^-1_infty(tau)) = tau","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"This theorem is very simple, but it is useful because it suggest a simple path to showing the consistency of the bootstrap: simply show that T_n^ast has the same asymptotic distribution as T_n. Here is a simple result for when T_n^ast is constructed by sampling with replacement from the empirical distribution. We will let mathbbP_n denote the empirical distribution, and x_i^ast denote draws of x_i from it.","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"tip: Lemma\nLet x_1 x_2  be i.i.d. with mean mu and variance sigma^2. Then conditional on x_1  x_n for almost every sequencesqrtn (barx_n^ast - barx_n) leadsto N(0sigma^2)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"info: Proof sketch:\nIt is straightforward to show that mathrmEx_i^ast  mathbbP_n = barx_n and ``Var(xi^\\ast|\\mathbb{P}n) = \\bar{x^2}_n\\bar{x}_n^2 \\to \\sigma^2``. Applying the Lindeberg CLT then gives theresult.","category":"page"},{"location":"bootstrap/#Pivotal-statistics","page":"Bootstrap","title":"Pivotal statistics","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"The above results imply that the bootstrap works for both  S_n = sqrtn(barx_n - mu_0) and \"studentized\" a statistic  T_n = sqrtn(barx_n - mu_0)hatsigma_n. There is some advantage to using the later. A statistic is called pivotal if its distribution is completely known. If we assume x_i sim N, then T_n is pivotal and has a t-distribution. If we aren't willing to assume normality, then the distribution of T_n is unknown, but its asymptotic distribution is completely known, N(01). Such a statistic is called asymptotically pivotal. S_n is not asymptotically pivotal because its asymptotic distribution depends on the unknown variance. It is possible to show that the bootstrap distribution of asymptotically pivotal statistics converge faster than either the usual asymptotic approximation or the bootstrap distribution of non-pivotal statistics. See (Hall, 1994) for details. ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Here is a simulation to illustrate. ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Random.seed!(14)\ndgp(n) = rand(Exponential(),n)\nestimator(x) = mean(x)\nθ0 = 1.0\nN = [5 10 20 100]\nB = 999\nfunction simulatetests(n)\n  function bootstrap(data, stat, S)\n    n = length(data)\n    T = [stat(sample(data,n)) for s in 1:S]\n  end\n  data = dgp(n)\n  t = sqrt(n)*(mean(data)-θ0)/std(data)\n  [cdf(Normal(),t),\n   mean(t.<bootstrap(data, d->(sqrt(n)*(mean(d) - mean(data))/std(d)), B)),\n   mean((mean(data)-θ0) .< bootstrap(data, d->(mean(d)-mean(data)), B))]\nend\nres=[hcat([simulatetests.(n) for s in 1:1000]...) for n in N]\n\np = 0:0.01:1\nglobal fig = plot(layout=4, legend=:none)\nfor i=1:4\n  plot!(fig,p, p->(mean(res[i][1,:].<p)-p), title=\"N=$(N[i])\",\n        label=\"asymptotic\", subplot=i)\n  \n  plot!(fig,p, p->(mean(res[i][2,:].<p)-p), title=\"N=$(N[i])\",\n        label=\"pivotal bootstrap\", subplot=i)\n  plot!(fig,p, p->(mean(res[i][3,:].<p)-p), title=\"N=$(N[i])\",\n        label=\"non-pivotal boostrap\", subplot=i)\nend\nfig","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"This figure shows the simulated CDF of p-values minus p. For a perfectly sized test, the line would be identically 0. The blue lines are for the usual t-test. Green is from bootstrapping the non-pivotal statistic sqrtn(barx^ast - barx). Red is from bootstrapping a pivotal t-statistic. As expected, the red line is closer to 0, illustrating the advantage of bootstrapping a pivotal statistic. ","category":"page"},{"location":"bootstrap/#Bootstrap-does-not-always-work","page":"Bootstrap","title":"Bootstrap does not always work","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"It is important to remember that the bootstrap is not guaranteed to work. A classic example is estimating the mean squared.  Let x_i sim F_0, whereF_0 is any distribution with mean mu and variance sigma^2. The parameter of interest is theta = mu^2. The estimator will be hattheta = barx^2.  The delta method and CLT imply ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"    sqrtn(barx^2 - mu^2) leadsto 2mu N(0sigma^2)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"similarly conditional on the data,","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"    sqrtn(barx^ast^2 - barx^2) leadsto 2 mu N(0sigma^2)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"A problem occurs when mu=0. The limiting distributions become point masses at 0. The CDF is no longer continuous, so the theorem above does not apply. ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Here's an illustration ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Random.seed!(622)\nfunction bootmeansquared(μ0, n)\n  dgp(n) = rand(n) .- 0.5 .+ μ0\n  estimator(x) = mean(x)^2  \n  S = 1000\n  T = [estimator(dgp(n)) for s in 1:S] .- μ0^2\n  function bootstrap(data, estimator, S)\n    n = length(data)\n    θhat = estimator(data)\n    [estimator(sample(data,n,replace=true)) for s in 1:S] .- θhat \n  end\n  Tboot = bootstrap(dgp(n),estimator, S)\n  density(T, label=\"true distribution\")\n  density!(Tboot, label=\"bootstrap distribution\")\nend\nbootmeansquared(0.5,100)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"bootmeansquared(0.0,500)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Depending on the random numbers drawn (in particular, whether the simulated sample mean is very close to 0 or not), the above picture may look okay or terrible for the bootstrap. Try running it a few times to get a sense of how bad it might be.","category":"page"},{"location":"bootstrap/#Bootstrap-for-GMM","page":"Bootstrap","title":"Bootstrap for GMM","text":"","category":"section"},{"location":"bootstrap/#Joint-hypotheses","page":"Bootstrap","title":"Joint hypotheses","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"The failure of the bootstrap for the mean squared when the true mean is 0 has important implications for GMM. In particular, the AR statistic, ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"AR(theta) = n 1n sum g_i(theta) widehatVar(g_i(theta))^-1\n1n sum g_i(theta) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"is essentially a mean squared. If we naively attempt to apply the bootstrap by computing","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"AR(theta)^ast = n 1n sum g_i^ast(theta) widehatVar(g_i^ast(theta))^-1\n1n sum g_i^ast(theta) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"we will get incorrect inference. The problem is that we want to test H_0 mathrmEg_i(theta) = 0, but in the bootstrap sample, ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"mathrmEg^ast(theta)data = 1n sum g_i(theta) neq 0","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"For the bootstrap to work, we must ensure that the null hypothesis is true in the bootstrap sample, we can do this by taking","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"AR(theta)^ast = n 1n sum g_i^ast(theta) - barg_n(theta) widehatVar(g_i^ast(theta))^-1\n1n sum g_i^ast(theta) - barg_n(theta) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"where barg_n(theta) = 1n sum g_i(theta). ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Here is a simulation to illustrate. It uses the same IV-logit share example as in the extremum estimation notes.","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"n = 100\nk = 2\niv = 3\nβ0 = ones(k)\nπ0 = vcat(I,ones(iv-k,k))\nρ = 0.5\nRandom.seed!(622)\ndata = IVLogitShare(n,β0,π0,ρ)\n\nfunction arstat(gi)\n  n = size(gi)[1]\n  gn = mean(gi,dims=1) \n  W = pinv(cov(gi))\n  n*( gn*W*gn')[1]\nend\n\nfunction ar(θ,gi)\n  n,m = size(gi(θ))\n  1.0-cdf(Chisq(m), arstat(gi(θ)))\nend\n\nfunction bootstrapAR(θ,gi)\n  giθ = gi(θ)\n  gn = mean(giθ,dims=1)\n  n = size(giθ)[1]\n  S = 999\n  T =hcat([ [arstat(giθ[sample(1:n,n,replace=true),:]),\n             arstat(giθ[sample(1:n,n,replace=true),:].-gn)\n             ] for s in 1:S]...)'\n  t = arstat(giθ)\n  [1-cdf(Chisq(length(gn)),t)  mean(T.>=t, dims=1)]\nend\n\nfunction simulatetests()\n  data = IVLogitShare(n,β0,π0,ρ)\n  bsp=bootstrapAR(β0, get_gi(data))\nend","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"pvals = vcat([simulatetests() for s in 1:500]...)\np = 0:0.01:1.0\nplot(p, p->(mean(pvals[:,1].<p)-p),  label=\"asymptotic\", legend=:bottomleft)\nplot!(p, p->(mean(pvals[:,2].<p)-p),  label=\"incorrect bootstrap\")\nplot!(p, p->(mean(pvals[:,3].<p)-p),  label=\"correct bootstrap\")","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"With the same sort of modification the KLM and CLR statistics can also be bootstrapped. All three statistics remain identification robust when bootstrapped. See Kleibergen (2006)[@kleibergen2006] for details.","category":"page"},{"location":"bootstrap/#Single-coefficients","page":"Bootstrap","title":"Single coefficients","text":"","category":"section"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"If we want to construct a confidence interval for a single coefficient, we can apply the bootstrap to a statistic like  sqrtn (hattheta - theta_0) (or a studentized version of it). Just like in the previous subsection, we must be careful to ensure that the null hypothesis holds in the bootstrapped data. Also, as above, we can do this by subtracting 1nsum_i g_i(hattheta) from the bootstrapped moments. Thus, one way to bootstrap is  sqrtn (hattheta - theta_0) is to","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Compute hattheta, barg_n(hattheta) = 1nsum_i g_i(hattheta)\nDraw with replacement  g_i^ast(theta) from g_i(theta) - barg_n(hattheta)_i=1^n. \nCompute ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"hattheta^ast = argmin 1n sum_i g_i^ast(theta) W_n^ast(theta)\n  1n sum g_i^ast(theta","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Use distribution of sqrtn (hattheta^ast - hattheta) to approximate sqrtn (hattheta - theta_0)","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"For some models, the minimazation needed to compute hattheta^ast can be very time consuming. Fortunately, it can be avoided. A key step in showing that hattheta is asymptotically normal is a linearization","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"sqrtn(hattheta - theta_0) = -(DWD)^-1 (D W)\nfrac1sqrtn sum_i g_i(theta_0) + o_p(1) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Showing the bootstrap estimator is asymptotically normal conditional on the data involves a similar linearization","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"sqrtn(hattheta^ast - hattheta) = -(hatDhatWhatD)^-1 (hatD hatW)\nfrac1sqrtn sum_i g_i^ast(hattheta) + o_p(1) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"This suggests taking ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"hattheta^ast = hattheta + (hatDhatWhatD)^-1 (hatD hatW)\nfrac1sqrtn sum_i g_i^ast(hattheta) ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"instead of re-minimizing. ","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"function gmmVar(θ,gi,W)\n  g = gi(θ)\n  n = size(g)[1]\n  D = ForwardDiff.jacobian(θ->mean(gi(θ),dims=1),θ)\n  Σ = cov(gi(θ))\n  1/n*inv(D'*W*D)*(D'*W*Σ*W*D)*inv(D'*W*D)\nend\n\nfunction bsweights(n)\n  s = sample(1:n,n)\n  (x->sum(x.==s)).(1:n)\nend\n\n\nfunction bootstrapt(θ,gi, W)\n  giθ = gi(θ)\n  gn = size(giθ)[1]\n  D = ForwardDiff.jacobian(θ->mean(gi(θ),dims=1),θ) \n  function bootonce(θ,gi)\n    giθ = gi(θ)\n    n = size(giθ)[1]\n    gn = mean(giθ,dims=1)\n    w = bsweights(n)\n    giw(θ) = (gi(θ).*w .- gn)\n    gmmobj = gmm_objective(giw, W)\n    opt1 = optimize(gmmobj, θ, BFGS(), autodiff =:forward)\n    θs1 = opt1.minimizer\n    θs2 = θ - inv(D'*W*D)*(D'*W*( (mean(giθ.*w,dims=1).-gn)' ))\n    [θs1[1] θs2[1]]\n  end\n  S = 299\n  vcat([bootonce(θ,gi) for s in 1:S]...)\nend","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"n = 50\nk = 2\niv = 3\nβ0 = ones(k)\nπ0 = vcat(I,ones(iv-k,k))\nρ = 0.5\nRandom.seed!(622)\ndata = IVLogitShare(n,β0,π0,ρ)\ngi = get_gi(data)\nW = I\noptres = optimize(gmm_objective(data,W), β0, BFGS(),\n                  autodiff = :forward)\nθhat = optres.minimizer\nθs = bootstrapt(θhat,gi,I)\ndensity(θs[:,1], label=\"minimization bootstrap\")\ndensity!(θs[:,2], label=\"score bootstrap\")","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"function simulatetests()\n  data = IVLogitShare(n,β0,π0,ρ)\n  gi = get_gi(data)\n  W = I\n  optres = optimize(gmm_objective(gi,W), β0, BFGS(), autodiff =:forward)\n  θhat = optres.minimizer \n  θs = bootstrapt(θhat,gi,I)\n  p = mean((θhat[1]-β0[1]).<(θs .- θhat[1]), dims=1)\nend\nRandom.seed!(622)\npvals = vcat([simulatetests() for s in 1:300]...)\np = 0:0.01:1.0\nfig=plot(p, p->(mean(pvals[:,1].<p)-p),\n         label=\"minimization bootstrap\", legend=:best)\nplot!(fig,p, p->(mean(pvals[:,2].<p)-p),  label=\"score bootstrap\")\nfig","category":"page"},{"location":"bootstrap/","page":"Bootstrap","title":"Bootstrap","text":"Both versions of the bootstrap appear to work well enough here. Note that neither one is robust to identification problems. Inference on a subset of parameters while remaining robust to identification problems is somewhat of an open problem. Various conservative approaches are available, the simplest of which is to just take projections of the AR, KLM, or CLR confidence sets. There are also more powerful approaches for situations where you know which parameters are strongly vs weakly identified. ","category":"page"},{"location":"identificationRobustInference/#Identification-robust-inference","page":"Identification Robust Inference","title":"Identification robust inference","text":"","category":"section"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"defindepperpperp\ndefErmathrmE\ndefRmathbbR\ndefEnmathbbE_n\ndefPrmathrmP\ndefinprobbuildrel p over rightarrow \ndefindistbuildrel d over rightarrow \n","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"As discussed in section 9 of (Newey and McFadden, 1994), there are three classic types of statistics for testing restrictions on parameters. Suppose you want to test H_0 a(theta) = 0. Let hattheta denote the unrestricted estimate, and let hattheta^r denote the estimate of theta subject to the restriction. Wald test-statistics are based on hattheta-hattheta^r. Lagrange multiplier tests look at the distribution of the estimated Lagrange multiplier. Likelihood ratio (aka distance metric in (Newey and McFadden, 1994)) tests look at Q_n(hattheta) - Q_n(hattheta^r). If we consider testing H_0 theta = vartheta for some fixed vartheta, then the usual approach based on the asymptotic normality of hattheta discussed above is exactly the same as the Wald test of this restriction. As discussed by (Newey and McFadden, 1994), under standard assumptions, all three testing approaches are asymptotically equivalent. However, the tests can and will differ in finite samples. More importantly, in the face of identification problems, Wald tests tend to break down, while Lagrange multiplier and likelihood ratio style tests can continue to work. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"By identification robust, we mean an inference procedure that has correct size regardless of whether identification is strong, weak, or partial. In the asymptotic normality of extremum estimators theorem above, non-strong identification will create problems for assumption 3, in particular the assumption that the Hessian is non-singular. For this section, we will focus on GMM estimators. Identification problems most often arrive and have been studied in the context of GMM. Also, it is not difficult to transform other extremum estimators into GMM. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"For a GMM objective function of the form:","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":" left(frac1n sum_i g_i(theta)right) W_n left(frac1n sum g_i(theta)right)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"if we assume:","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"1sqrtn sum_i g_i(theta_0) leadsto N(0Sigma)\n1n sum_i nabla g_i(theta) to^p Enabla g(theta)equiv D,  W_n to^p W\n(DWD) is nonsingular.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"then the above theorem for asymptotic normality of extremum estimators implies that ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"sqrtn(hattheta - theta_0) leadsto N(0Omega)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"where ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":" Omega= (DWD)^-1 (D W Sigma W D) (DWD)^-1","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"If we additionally assume W_n to^p Sigma^-1, e.g. observations are independent and W_n = widehatVar(g_i(theta))^-1, then the asymptotic variance simplifies to (D Sigma^-1 D)^-1. ","category":"page"},{"location":"identificationRobustInference/#Anderson-Rubin-test","page":"Identification Robust Inference","title":"Anderson-Rubin test","text":"","category":"section"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"As already stated, the assumption that (DWD) is nonsingular is problematic if we want to allow for identification problems. However, if we assume only that ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"frac1sqrtn sum_i g_i(theta_0) leadsto N(0Sigma)\nW_n to^p Sigma^-1","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"then ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"n left(frac1n sum_i g_i(theta)right) W_n left(frac1n sum g_i(theta)right)\n  leadsto chi^2_m","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"where m is the number of moments (dimension of g_i(theta)). This is called the Anderson-Rubin test. Note that this result holds without any explicit nonsingular assumption about a Hessian. Hence, there is hope that this result would be true even with identification problems. Indeed, it is. (Stock and Wright, 2000) first proposed using this test statistic for weakly identified GMM estimators. (Stock *et al.*, 2002) gives an overview of this test and related tests with a focus on linear IV. (Caner, 2009) discusses this test in the context of GMM.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Typical usage of the AR test is to invert the test to construct a confidence region for theta. For each theta in Theta,  let ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"AR(theta) = n 1n sum g_i(theta) widehatVar(g_i(theta))^-1\n1n sum g_i(theta) ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"and let c_alpha= alpha quantile of chi^2_m. Then a alpha confidence region for theta_0 is","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":" theta in Theta AR(theta) leq c_alpha ","category":"page"},{"location":"identificationRobustInference/#Example:-IV-logit-demand","page":"Identification Robust Inference","title":"Example: IV logit demand","text":"","category":"section"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"A common way to model demand for differentiated products is to aggregate an individual discrete choice. We will look at the simplest such model here. This is a model for when we have data on product market shares, y_j, and product attributes, x_j, for many different markets. In concrete applications, markets may be defined geographically, temporally, by consumer segment, or some combination thereof. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Consider a single good, which consumers chooose to purchase or not. Consumer i's utility from consuming the good is","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"u_ij = x_j beta + xi_j + epsilon_ij","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"where x_j are the observed attributes of the good in market j, xi_j is a market level demand shock, and epsilon_ij is an individual taste shock. Person i purchases the good if  u_ij geq 0. Aggregating individual purchases implies that the market share in market j is ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"y_j = F_-epsilon(x_j beta + xi_j) ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"where F_-epsilon is the CDF of -epsilon.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"We assume that epsilon_ij is independent of x_j and xi_j. Typically, x_j includes some easily adjusted product attributes, such as price, so we want to allow x_j to be correlated with xi_j. Assume that we have some instruments z_j such that mathrmExi_j z_j=0  We can write this moment condition in terms of observables and beta as","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"mathrmE (F^-1_-epsilon(y_j) - x_jbeta) z_j  = 0","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"This is the moment condition we will use to estimate beta.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"First, we will simulate the model, then estimate it. This code looks at three variants of GMM. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"First, it computes an estimate with W_n = I. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Second, it computes an efficiently weighted estimated with ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"W_n = widehatVar(g_i(hattheta_(1)))","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"where hattheta_(1) is the first estimate. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Third, it computes the continuously updating estimator, which uses AR(theta) as the objective function ( W is \"continuously updated\" to be widehatVar( g_i(theta)) ).","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"using Optim, ForwardDiff, LinearAlgebra, Distributions\nimport Random\nfunction simulate_ivshare(n,β,γ,ρ)\n  z = randn(n, size(γ)[1])\n  endo = randn(n, length(β))\n  x = z*γ .+ endo\n  ξ = rand(Normal(0,sqrt((1.0-ρ^2))),n).+endo[:,1]*ρ \n  y = cdf.(Logistic(), x*β .+ ξ)\n  return((y=y,x=x,z=z))  \nend\nn = 100\nk = 2\niv = 3\nβ0 = ones(k)\nπ0 = vcat(5*I,ones(iv-k,k)) \nρ = 0.5  \nRandom.seed!(622)\n(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\n\nfunction gi_ivshare(β,y,x,z)\n  ξ = quantile.(Logistic(),y) .- x*β\n  ξ.*z\nend\n\nfunction gmmObj(θ,gi,W)\n  g = gi(θ)\n  m = mean(g,dims=1)\n  (size(g)[1]*( m*W*m')[1]) # return scalar, not 1x1 array\nend\n\nfunction gmmVar(θ,gi,W)\n  g = gi(θ)\n  n = size(g)[1]\n  D = ForwardDiff.jacobian(θ->mean(gi(θ),dims=1),θ)\n  Σ = cov(gi(θ))\n  1/n*inv(D'*W*D)*(D'*W*Σ*W*D)*inv(D'*W*D)\nend\n\nfunction ar(θ,gi)\n  gmmObj(θ,gi,inv(cov(gi(θ))))\nend","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"opt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                zeros(k), BFGS(), autodiff =:forward)\n@show β1 = opt1.minimizer\ndisplay(gmmVar(β1, β->gi_ivshare(β,y,x,z),I))\nopteff = optimize(θ->gmmObj(θ,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(β1,y,x,z)))),\n                  zeros(k), BFGS(), autodiff =:forward)\n@show βeff = opteff.minimizer\ndisplay(gmmVar(βeff,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(β1,y,x,z)))))\n\nar_ivshare = θ->ar(θ,β->gi_ivshare(β,y,x,z))\noptcue = optimize(ar_ivshare,\n                  β0, BFGS(), autodiff =:forward)\n@show βcue = optcue.minimizer\nVcue = gmmVar(βcue,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))\ndisplay(Vcue)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Now we compare confidence regions based on the Wald test, and from inverting the AR statistic.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"using Plots, LaTeXStrings\nPlots.gr()\nfunction plot_cr(β,V, AR)\n  lb = β - sqrt.(diag(V))*5\n  ub = β + sqrt.(diag(V))*5\n  ntest = 1000\n  βtest = [rand(length(β)).*(ub-lb) .+ lb for i in 1:ntest]\n  arstat = AR.(βtest)\n  βtest = vcat(βtest'...)\n  crit = quantile(Chisq(size(z)[2]), 0.9)\n  scatter(βtest[:,1],βtest[:,2], group=(arstat.<crit), legend=false,\n          markersize=4, markerstrokewidth=0.0, seriesalpha=0.8,\n          xlabel=L\"\\beta_1\", ylabel=L\"\\beta_2\")\n  scatter!([β0[1]], [β0[2]], markersize=8)\n  b1 = lb[1]:(ub[1]-lb[1])/100:ub[1]\n  b2 = lb[2]:(ub[2]-lb[2])/100:ub[2]\n  arf = (a,b) -> cdf(Chisq(size(z)[2]),AR([a,b]))\n  contour!(b1,b2,arf, levels = [0.9, 0.95],\n           contour_labels=false, legend=false,\n           label=\"AR CI\",\n           c=cgrad([:black,:black],[0.0,1.0]))\n  waldf = (a,b) -> cdf(Chisq(length(βcue)),([a,b]-βcue)'*inv(Vcue)*([a,b]-βcue))\n  contour!(b1,b2,waldf, levels = [0.9, 0.95],\n           contour_labels=false,\n           label=\"Wald CI\", c=cgrad([:red,:red], [0.0,1.0]),\n           legend=false)\nend\nplot_cr(βcue,Vcue, ar_ivshare)\n","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"The two confidence regions above are not too different because the simulated data was strongly identified. Let's see what happens when we change the simulation to have weaker identification.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"# make π0 nearly rank-deficient\nπ0 = ones(iv,k) .+ vcat(I*0.05,zeros(iv-k,k))  \nρ = 0.5  \nRandom.seed!(14)\n(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\nar_ivshare = θ->ar(θ,β->gi_ivshare(β,y,x,z))\noptcue = optimize(ar_ivshare,\n                  β0, BFGS(), autodiff =:forward)\n@show βcue = optcue.minimizer\nVcue = gmmVar(βcue,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))\ndisplay(Vcue)\nplot_cr(βcue,Vcue, ar_ivshare)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Now the confidence regions are dramatically different. Does either one have correct coverage? Let's simulate to find the size of the AR and Wald tests of H_0  beta = beta_0","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"S = 500\nn = 150\nfunction sim_p(π0)\n  (y,x,z) = simulate_ivshare(n,β0,π0,ρ)\n  opt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                  β0, BFGS(), autodiff =:forward)\n  if (!opt1.g_converged)\n    opt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                    β0, NewtonTrustRegion(), autodiff =:forward)\n  end\n  β1 = opt1.minimizer\n\n  V1 = gmmVar(β1,β->gi_ivshare(β,y,x,z),I)\n  \n  optcue = optimize(θ->ar(θ,β->gi_ivshare(β,y,x,z)),\n                    β0, BFGS(), autodiff =:forward)\n  if (!optcue.g_converged)\n    optcue = optimize(θ->ar(θ,β->gi_ivshare(β,y,x,z)),\n                      β0, NewtonTrustRegion(), autodiff =:forward)\n  end\n  if (!optcue.g_converged)\n    display(optcue)\n    βcue = deepcopy(β1)\n  else \n    βcue = optcue.minimizer\n  end\n  Vcue =\n    gmmVar(βcue,β->gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))\n  arp = θ->(1.0-cdf(Chisq(size(z)[2]),ar(θ,β->gi_ivshare(β,y,x,z))))\n  waldp = θ->(1.0-cdf(Chisq(length(βcue)),(θ-βcue)'*inv(Vcue)*(θ-βcue)))\n  waldp1 = θ->(1.0-cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1)))\n  [arp(β0) waldp(β0) waldp1(β0)]\nend\nπweak = ones(iv,k) .+ vcat(diagm(0=>fill(0.01,k)),zeros(iv-k,k))  \nπstrong = vcat(5*diagm(0=>ones(k)),ones(iv-k,k)) \n@time pweak=vcat([sim_p(πweak) for s in 1:S]...)\n@time pstrong=vcat([sim_p(πstrong) for s in 1:S]...)\n\npgrid = 0:0.01:1\nplot(pgrid, p->mean( pstrong[:,1] .<= p)-p, legend=:bottomleft,\n     label=\"AR, strong ID\", style=:dash, color=:red,\n     xlabel=\"p\", ylabel=\"P(p value < p) - p\",\n     title=\"Simulated size distortion\") \nplot!(pgrid, p->mean( pstrong[:,2] .<= p)-p,\n      label=\"Wald CUE, strong ID\", style=:dash, color=:blue)\nplot!(pgrid, p->mean( pstrong[:,3] .<= p)-p,\n      label=\"Wald I, strong ID\", style=:dash, color=:green)\n\nplot!(pgrid, p->mean( pweak[:,1] .<= p)-p,\n      label=\"AR, weak ID\", style=:solid, color=:red)\nplot!(pgrid, p->mean( pweak[:,2] .<= p)-p,\n      label=\"Wald CUE, weak ID\", style=:solid, color=:blue)\nplot!(pgrid, p->mean( pweak[:,3] .<= p)-p,\n      label=\"Wald I, weak ID\", style=:solid, color=:green)\n","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"We see that the Wald test has fairly large size distortions, even when identification is strong. The AR test has approximately correct size for both the weakly and strongly identified DGP. ","category":"page"},{"location":"identificationRobustInference/#Other-identification-robust-tests","page":"Identification Robust Inference","title":"Other identification robust tests","text":"","category":"section"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"There are also identification robust versions of likelihood ratio and lagrange multiplier test. (Moreira, 2003) proposed a conditional likelihood ratio (CLR) test for weakly identified linear IV models. (Kleibergen, 2005) developed a Lagrange multiplier (often called the KLM) test and extended Moreira's CLR test to weakly identified GMM models.  More recently, (Andrews and Guggenberger, 2015) and (Andrews and Guggenberger, 2017) showed the validity of these tests under more general conditions. These tests are somewhat more complicated than the AR test, but they have the advantage that they are often more powerful. The AR test statistic has a \\chi^2{m} distribution, where m is the number of moments. The CLR and KLM statistics under strong identification have \\chi^2k distributions (as does the Wald statistic), where k is the number of parameters. Consequently, when the model is overidentified, the CLR and LM tests are more powerful than the AR test. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Here is an implementation of the KLM and CLR statistics. The names of variables roughly follows the notation of (Andrews and Guggenberger, 2017).","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"using ForwardDiff, Plots, Optim\nPlots.gr()\nfunction statparts(θ,gi)\n  # compute common components of klm, rk, & clr stats\n  # follows notation of Andrews & Guggenberger 2017, section 3.1\n  function P(A::AbstractMatrix) # projection matrix\n    A*pinv(A'*A)*A'\n  end\n  giθ = gi(θ)\n  p = length(θ)    \n  (n, k) = size(giθ)\n  Ω = cov(giθ)  \n  gn=mean(gi(θ), dims=1)'\n  #G = ForwardDiff.jacobian(θ->mean(gi(θ),dims=1),θ)\n  Gi= ForwardDiff.jacobian(gi,θ)\n  Gi = reshape(Gi, n , k, p)\n  G = mean(Gi, dims=1)\n  Γ = zeros(eltype(G),p,k,k)\n  D = zeros(eltype(G),k, p)\n  for j in 1:p\n    for i in 1:n\n      Γ[j,:,:] += (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'\n    end\n    Γ[j,:,:] ./= n\n    D[:,j] = G[1,:,j] - Γ[j,:,:]*inv(Ω)*gn\n  end\n  return(n,k,p,gn, Ω, D, P)\nend\n\nfunction klm(θ,gi)\n  (n,k,p,gn, Ω, D, P) = statparts(θ,gi)\n  lm = n*(gn'*Ω^(-1/2)*P(Ω^(-1/2)*D)*Ω^(-1/2)*gn)[1]\nend\n\nfunction clr(θ,gi)\n  (n,k,p,gn, Ω, D, P) = statparts(θ,gi)\n  \n  rk = eigmin(n*D'*inv(Ω)*D)\n  AR  = (n*gn'*inv(Ω)*gn)[1]\n  lm = (n*(gn'*Ω^(-1/2)*P(Ω^(-1/2)*D)*Ω^(-1/2)*gn))[1]  \n  lr = 1/2*(AR - rk + sqrt( (AR-rk)^2 + 4*lm*rk))\n  \n  # simulate to find p-value\n  S = 5000\n  function randc(k,p,r,S)\n    χp = rand(Chisq(p),S)\n    χkp = rand(Chisq(k-p),S)\n    0.5.*(χp .+ χkp .- r .+\n          sqrt.((χp .+ χkp .- r).^2 .+ 4 .* χp.*r))\n  end\n  csim = randc(k,p,rk,S)\n  pval = mean(csim.<=lr)\nend","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"function plot_cr(β,V, tests::AbstractArray{Function}, labels; ngrid=30)\n  lb = β - sqrt.(diag(V))*5\n  ub = β + sqrt.(diag(V))*5\n  fig=scatter([β0[1]], [β0[2]], markersize=8, legend=false,\n              xlabel=L\"\\beta_1\", ylabel=L\"\\beta_2\")\n  ntest = 1000\n  βtest = [rand(2).*(ub-lb) .+ lb for i in 1:ntest]\n  pval = tests[1].(βtest)\n  βtest = vcat(βtest'...)\n  crit = 0.9\n  fig=scatter!(βtest[:,1],βtest[:,2], group=(pval.<crit), legend=false,\n               markersize=4, markerstrokewidth=0.0, seriesalpha=0.5,\n               palette=:heat)\n  b1 = lb[1]:(ub[1]-lb[1])/ngrid:ub[1]\n  b2 = lb[2]:(ub[2]-lb[2])/ngrid:ub[2]\n  colors = [:black, :red, :blue, :green]\n  for t in eachindex(tests)\n    fig=contour!(b1,b2,(a,b)->tests[t]([a,b]),\n             levels = [0.9, 0.95],\n             contour_labels=false, legend=false,\n             label = labels[t],\n             c=cgrad([colors[t],colors[t]],[0.0,1.0]))\n  end\n  fig\nend","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Here's what the confidence regions look like when identification is fairly weak. The green lines are the Wald confidence region, blue is AR, red is KLM, and black is CLR. ","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"n = 50\nk = 2\niv =3 \nπ0 = vcat(0.1*diagm(0=>ones(k)),0.2*ones(iv-k,k)) \nρ = 0.5  \nRandom.seed!(622)\n(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\nopt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                β0, BFGS(), autodiff =:forward)\nβ1 = opt1.minimizer\nV1 = gmmVar(β1,β->gi_ivshare(β,y,x,z),I)\n  \n\npklm = θ->cdf(Chisq(length(βcue)),klm(θ, β->gi_ivshare(β,y,x,z)))\npar  = θ->cdf(Chisq(size(z)[2]), ar(θ, β->gi_ivshare(β,y,x,z)))\npclr  = θ->clr(θ, β->gi_ivshare(β,y,x,z))\npwald = θ -> cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))\nplot_cr(β1,V1, [pclr, pklm, par, pwald],\n        [\"CLR\",\"KLM\",\"AR\",\"Wald\"], ngrid=40)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Here's what the confidence regions look like when identification is stronger.","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"n = 50\nk = 2\niv =3 \nπ0 = vcat(3*diagm(0=>ones(k)),ones(iv-k,k)) \nρ = 0.5  \nRandom.seed!(622)\n(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\nopt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                β0, BFGS(), autodiff =:forward)\nβ1 = opt1.minimizer\nV1 = gmmVar(β1,β->gi_ivshare(β,y,x,z),I)\n  \n\npklm = θ->cdf(Chisq(length(βcue)),klm(θ, β->gi_ivshare(β,y,x,z)))\npar  = θ->cdf(Chisq(size(z)[2]), ar(θ, β->gi_ivshare(β,y,x,z)))\npclr  = θ->clr(θ, β->gi_ivshare(β,y,x,z))\npwald = θ -> cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))\nplot_cr(β1,V1, [pclr, pklm, par, pwald],\n        [\"CLR\",\"KLM\",\"AR\",\"Wald\"], ngrid=40)","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"Check the size","category":"page"},{"location":"identificationRobustInference/","page":"Identification Robust Inference","title":"Identification Robust Inference","text":"S = 300\nn = 100\nfunction sim_p(π0)\n  (y,x,z) = simulate_ivshare(n,β0,π0,ρ)\n  opt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                  β0, BFGS(), autodiff =:forward)\n  if (!opt1.g_converged)\n    opt1 = optimize(θ->gmmObj(θ, β->gi_ivshare(β,y,x,z) ,I),\n                    β0, NewtonTrustRegion(), autodiff =:forward)\n  end\n  β1 = opt1.minimizer\n  V1 = gmmVar(β1,β->gi_ivshare(β,y,x,z),I)\n  \n  pklm = θ->cdf(Chisq(length(β1)),klm(θ, β->gi_ivshare(β,y,x,z)))\n  par  = θ->cdf(Chisq(size(z)[2]), ar(θ, β->gi_ivshare(β,y,x,z)))\n  pclr  = θ->clr(θ, β->gi_ivshare(β,y,x,z))\n  pwald = θ -> cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))  \n  [par(β0) pwald(β0) pclr(β0) pklm(β0)]\nend\nπweak = ones(iv,k) .+ vcat(diagm(0=>fill(0.01,k)),zeros(iv-k,k))  \nπstrong = vcat(3*diagm(0=>ones(k)),ones(iv-k,k)) \npweak=vcat([sim_p(πweak ) for s in 1:S]...)\npstrong=vcat([sim_p(πstrong) for s in 1:S]...)\n\npgrid = 0:0.01:1\nplot(pgrid, p->mean( pstrong[:,1] .<= p)-p, legend=:topleft,\n     label=\"AR, strong ID\", style=:dash, color=:blue,\n     xlabel=\"p\", ylabel=\"P(p value < p)\",\n     title=\"Simulated size distortion\") \nplot!(pgrid, p->mean( pstrong[:,2] .<= p)-p,\n      label=\"Wald I, strong ID\", style=:dash, color=:green)\nplot!(pgrid, p->mean( pstrong[:,3] .<= p)-p,\n      label=\"CLR, strong ID\", style=:dash, color=:black)\nplot!(pgrid, p->mean( pstrong[:,4] .<= p)-p,\n      label=\"KLM, strong ID\", style=:dash, color=:red)\n\nplot!(pgrid, p->mean( pweak[:,1] .<= p)-p,\n      label=\"AR, weak ID\", style=:solid, color=:blue)\nplot!(pgrid, p->mean( pweak[:,2] .<= p)-p,\n      label=\"Wald I, weak ID\", style=:solid, color=:green)\nplot!(pgrid, p->mean( pweak[:,3] .<= p)-p,\n      label=\"CLR, weak ID\", style=:solid, color=:black)\nplot!(pgrid, p->mean( pweak[:,4] .<= p)-p,\n      label=\"KLM, weak ID\", style=:solid, color=:red)","category":"page"},{"location":"#GMMInference.jl","page":"Autodocs","title":"GMMInference.jl","text":"","category":"section"},{"location":"#GMMModel","page":"Autodocs","title":"GMMModel","text":"","category":"section"},{"location":"","page":"Autodocs","title":"Autodocs","text":"The abstract type GMMModel is used to define methods for generic method of moments problems. To use this interface, define a concrete  subtype of GMMModel and at least a specialized get_gi method. See src/models for a few examples. ","category":"page"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"GMMInference.jl\"]","category":"page"},{"location":"#GMMInference.GMMInference","page":"Autodocs","title":"GMMInference.GMMInference","text":"module GMMInference\n\n\n\n\n\n","category":"module"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"gmmmodel.jl\"]","category":"page"},{"location":"#GMMInference.GMMModel","page":"Autodocs","title":"GMMInference.GMMModel","text":"GMMModel\n\nAbstract type for GMM models.\n\n\n\n\n\n","category":"type"},{"location":"#GMMInference.cue_objective-Tuple{Function}","page":"Autodocs","title":"GMMInference.cue_objective","text":" cue_objective(gi::Function)\n\nReturns the CUE objective function for moment functions gi. \n\nQ(θ) = n (1n sum_i g_i(θ)) widehatcov(g_i(θ))^-1 (1n sum_i g_i(θ))\n\nCalculates cov(gi(θ)) assuming observations are independent.\n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.cue_objective-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.cue_objective","text":"cue_objective(model::GMMModel)\n\nReturns the CUE objective function for model. \n\nCalculated weighting matrix assuming observations are independent.\n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.gel_jump_problem","page":"Autodocs","title":"GMMInference.gel_jump_problem","text":"gel_jump_problem(model::GMMModel, h::Function=log)\n\nReturns JuMP problem for generalized empirical likelihood estimation of model.  h is a generalized likelihood function. \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gel_nlp_problem","page":"Autodocs","title":"GMMInference.gel_nlp_problem","text":"gel_nlp_problem(model::GMMModel, h::Function=log)\n\nReturns NLPModel problem for generalized empirical likelihood estimation of model.  h is a generalized likelihood function. \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gel_optim_args","page":"Autodocs","title":"GMMInference.gel_optim_args","text":"gel_optim_args(model::GMMModel, h::Function=log)\n\nReturn tuple, out for calling optimize(out..., IPNewton()) afterward. \n\nIt seems that IPNewton() works better if the constraint on p is 0 ≤ sum(p) ≤ 1 instead of sum(p)=1, and you begin the optimizer from a point with sum(p) < 1. \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.get_gi-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.get_gi","text":"get_gi(model::GMMModel)\n\nReturns a function gi(θ) where the moment condition for a GMM model is\n\nEg_i(theta) = 0\n\ngi(θ) should be a number of observations by number of moment conditions matrix. \n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.gmm_constraints-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.gmm_constraints","text":"gmm_constraints(model::GMMModel)\n\nReturns constraint as function or parameters, θ, where c(θ) = 0.\n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.gmm_jump_problem","page":"Autodocs","title":"GMMInference.gmm_jump_problem","text":"gmm_jump_problem(model::GMMModel, obj=gmm_objective)\n\nConstructs JuMP problem for GMMModel. \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gmm_nlp_problem","page":"Autodocs","title":"GMMInference.gmm_nlp_problem","text":"gmm_nlp_problem(model::GMMModel, obj=gmm_objective)\n\nConstructs NLPModel for GMMModel. \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gmm_objective","page":"Autodocs","title":"GMMInference.gmm_objective","text":"gmm_objective(model::GMMModel, W=I)\n\nReturns the GMM objective function with weighting matrix W for model.\n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gmm_objective-2","page":"Autodocs","title":"GMMInference.gmm_objective","text":" gmm_objective(gi::Function, W=I)\n\nReturns Q(θ) = n (1n sum_i g_i(θ)) W (1n sum_i g_i(θ))\n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.number_moments-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.number_moments","text":"number_moments(model::GMModel)\n\nNumber of moments (columns of gi(θ)) for a GMMModel\n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.number_observations-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.number_observations","text":" number_observations(model::GMMModel)\n\nNumber of observations (rows of gi(θ)) for a GMMModel\n\n\n\n\n\n","category":"method"},{"location":"#GMMInference.number_parameters-Tuple{GMMModel}","page":"Autodocs","title":"GMMInference.number_parameters","text":" number_parameters(model::GMMModel)\n\nNumber of parameters (dimension of θ) for a GMMModel.\n\n\n\n\n\n","category":"method"},{"location":"#IV-Logit","page":"Autodocs","title":"IV Logit","text":"","category":"section"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"ivlogitshare.jl\"]","category":"page"},{"location":"#GMMInference.IVLogitShare","page":"Autodocs","title":"GMMInference.IVLogitShare","text":"IVLogitShare <: GMMModel\n\nAn IVLogitShare model consists of outcomes, y ∈ (0,1),  regressors x and instruments z.  The moment condition is\n\nE (log(y(1-y)) - xβ)z  = 0\n\nThe dimensions of x, y, and z must be such that length(y) == size(x,1) == size(z,1) and  size(x,2) ≤ size(z,2).\n\n\n\n\n\n","category":"type"},{"location":"#GMMInference.IVLogitShare-Tuple{Integer, AbstractVector, AbstractMatrix, Any}","page":"Autodocs","title":"GMMInference.IVLogitShare","text":"IVLogitShare(n::Integer, β::AbstractVector,\n                  π::AbstractMatrix, ρ)\n\nSimulate an IVLogitShare model. \n\nArguments\n\nn number of observations\nβ coefficients on x\nπ first stage coefficients x = z*π + v\nρ correlation between x[:,1] and structural error.\n\nReturns an IVLogitShare GMMModel.\n\n\n\n\n\n","category":"method"},{"location":"#Panel-Mixture","page":"Autodocs","title":"Panel Mixture","text":"","category":"section"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"mixturepanel.jl\"]","category":"page"},{"location":"#GMMInference.MixturePanel","page":"Autodocs","title":"GMMInference.MixturePanel","text":"MixturePanel(n::Integer, t::Integer,\n                  k::Integer, type_prob::AbstractVector,\n                  β::AbstractMatrix, σ = 1.0)\n\nSimulate a MixturePanel model.\n\nArguments\n\nn individuals\nt time periods\nk regressors\ntype_prob probability of each type\nβ matrix (k × lenght(type_prob)) coefficients\nσ standard deviation of ϵ\n\nReturns a MixturePanel GMMModel.\n\n\n\n\n\n","category":"type"},{"location":"#GMMInference.MixturePanel-2","page":"Autodocs","title":"GMMInference.MixturePanel","text":"MixturePanel <: GMMModel\n\nA MixturePanel model consists of outcomes, y, regressors, x, and a number of types, ntypes. Each observation i is one of ntypes types with probability p[i].  Conditional on type,y is given by\n\ny[i,t] = x[i,t,:]*β[:,type[i]] + ϵ[i,t]\n\nIt is assumed that ϵ is uncorrelated accross i and t and E[ϵ²]= σ².\n\nThe moment conditions used to estimate p, β and σ are\n\nE sum_j x(y - xβj)pj = 0\n\nand \n\nE yit*yis - sum_j pj(xitβj xisβj) -1(t=s)σ² = 0\n\n\n\n\n\n","category":"type"},{"location":"#Random-Coefficients-Logit","page":"Autodocs","title":"Random Coefficients Logit","text":"","category":"section"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"rclogit.jl\"]","category":"page"},{"location":"#GMMInference.RCLogit","page":"Autodocs","title":"GMMInference.RCLogit","text":"RCLogit <: GMMModel\n\nA random coefficients logit model with endogeneity.  An RCLogit model consists of outcomes, y ∈ (0,1),  regressors x, instruments z, and random draws ν ∼ N(0,I).  The moment condition is\n\nEξz = 0\n\nwhere \n\ny =  exp(x(β + ν) + ξ)(1 + exp(x(β + ν) + ξ)) dΦ(νΣ)\n\nwhere Φ(ν;Σ) is the normal distribution with variance Σ.\n\nThe dimensions of x, y, z, and ν must be such that length(y) == size(x,1) == size(z,1) == size(ν,2) and size(ν,3) == size(x,2) ≤ size(z,2). \n\n\n\n\n\n","category":"type"},{"location":"#GMMInference.RCLogit-2","page":"Autodocs","title":"GMMInference.RCLogit","text":"RCLogit(n::Integer, β::AbstractVector,\n        π::AbstractMatrix, Σ::AbstractMatrix,\n        ρ, nsim=100)\n\nSimulates a RCLogit model.\n\nArguments\n\nn number of observations\nβ mean coefficients on x\nπ first stage coefficients x = z*π + e\nΣ variance of random coefficients\nρ correlation between x[:,1] and structural error.\nnsim number of draws of ν for monte-carlo integration\n\n\n\n\n\n","category":"type"},{"location":"#GMMInference.gmm_constraints-Tuple{RCLogit}","page":"Autodocs","title":"GMMInference.gmm_constraints","text":"gmm_constraints(model::RCLogit)\n\nReturns \n\nc(θ) =  exp(x(β + ν) + ξ)(1 + exp(x(β + ν) + ξ)) dΦ(νΣ) - y\n\nwhere θ = [β, uvec, ξ] with uvec = vector(cholesky(Σ).U).\n\nThe integral is computed by monte-carlo integration. \n\n\n\n\n\n","category":"method"},{"location":"#Inference","page":"Autodocs","title":"Inference","text":"","category":"section"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Inference methods for GMM problems. Currently only contains GEL based tests. See empirical likelihood for usage, background, and references.","category":"page"},{"location":"","page":"Autodocs","title":"Autodocs","text":"Modules = [GMMInference]\nPages = [\"inference.jl\"]","category":"page"},{"location":"#GMMInference.gel_pλ","page":"Autodocs","title":"GMMInference.gel_pλ","text":"gel_pλ(model::GMMModel, h::Function=log)\n\nReturn a function that given parameters θ solves\n\nmax_p sum h(p) st sum(p) = 1 sum(p*gi(θ)) = 0\n\nThe returned function gives (p(θ),λ(θ))\n\nThe returned function is not thread-safe.  \n\n\n\n\n\n","category":"function"},{"location":"#GMMInference.gel_tests","page":"Autodocs","title":"GMMInference.gel_tests","text":"gel_tests(θ, model::GMMModel, h::Function=log)\n\nComputes GEL test statistics for H₀ : θ = θ₀. Returns a tuple containing statistics and p-values.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Autodocs","title":"Autodocs","text":"We plan to also add AR, KLM, and CLR methods for GMM. See identification robust inference. ","category":"page"}]
}
