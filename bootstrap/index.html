<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Bootstrap - GMMInference.jl</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">GMMInference.jl</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Package Documentation</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Notes and Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../extremumEstimation/" class="dropdown-item">Estimation</a>
</li>
                                    
<li>
    <a href="../identificationRobustInference/" class="dropdown-item">Inference</a>
</li>
                                    
<li>
    <a href="../empiricalLikelihood/" class="dropdown-item">Empirical Likelihood</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Bootstrap</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                            <li class="nav-item">
                                <a rel="prev" href="../empiricalLikelihood/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../license/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/GMMInference.jl/edit/master/docs/bootstrap.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#references" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#theory" class="nav-link">Theory</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#pivotal-statistics" class="nav-link">Pivotal statistics</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#bootstrap-does-not-always-work" class="nav-link">Bootstrap does not always work</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#bootstrap-for-gmm" class="nav-link">Bootstrap for GMM</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#joint-hypotheses" class="nav-link">Joint hypotheses</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#single-coefficients" class="nav-link">Single coefficients</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#references_1" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a> </p>
<h3 id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in
<a href="https://github.com/schrimpf/GMMInference.jl">on github</a>. The same
document generates both static webpages and associated jupyter
notebooks <a href="../bootstrap.ipynb">jupyter notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>The bootstrap is a method of inference that utilizes resampling. The
basic idea is as follows. Suppose you have some parameter of interest
for which you want to do inference. Let $T_n$ denote some test
statistic involving the estimator. The test 
statistic is a function of data, so the distribution of the estimator
is a function of the distribution of data. Let $F_0$
denote the exact, finite sample distribution of the data. Let 
<script type="math/tex; mode=display">
G_n(\tau, F_0) = \Pr(\hat{\theta}_n \leq \tau)
</script>
denote the exact finite sample distribution of the statistic. 
To do inference, we would like to know $G_n(\tau, F_0)$. 
This is generally impossible
without strong assumptions. Asymptotics get around this problem by
approximating $G_n(\tau, F_0)$ with its asymptotic distribution,
$G_\infty(\tau,F_0)$. The bootstrap is an alternative approach (but
the formal justification for the bootstrap still relies on
asymptotics). The bootstrap approximates $G_n(\tau, F_0)$ by replacing
$F_0$ with an estimate, $\hat{F}_n$. One common estimate of
$\hat{F}_n$ is simply the empirical CDF. When observations are
independent, we can randomly draw $T^\ast_n$ from 
$G_n(\tau, \hat{F}_n)$ by randomly drawing with replacement a sample
of size $n$ from the orgininal observations, and then computing
$T^\ast_n$ for this sample. We can do this repeatedly, and use
the distribution of the resulting $\hat{\theta}^\ast_n$&rsquo;s to calculate
$G_n(\tau,\hat{F}_n)$. </p>
<p>As a quick example, here&rsquo;s some code where the statistic is the sample
median minus its true value.</p>
<pre><code class="language-julia">using Plots, StatsPlots, Distributions, Optim, ForwardDiff, LinearAlgebra, GMMInference
Plots.gr();
</code></pre>
<pre><code class="language-julia">dgp(n) = rand(n)
estimator(x) = median(x)
# simulating T ~ G_n(τ,F_0)
n = 1000
S = 999
T = [estimator(dgp(n)) for s in 1:S] .- 0.5
function bootstrap(data, estimator, S)
  n = length(data)
  θhat = estimator(data)
  T = [estimator(sample(data,n, replace=true)) for s in 1:S] .- θhat 
end
Tboot = bootstrap(dgp(n),estimator, S)
density(T, label=&quot;true distribution&quot;)
density!(Tboot, label=&quot;bootstrap distribution&quot;)
</code></pre>
<p><img alt="" src="../figures/bootstrap_3_1.png" /></p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<p>Mackinnon (2006)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> and MacKinnon (2009)<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> are good practical introductions to
the bootstrap. Horowitz (2001)<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> is also a good overview, and includes
more precise statements of theoretical results, but does not contain
proofs. The lecture notes of Shi (2012)<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> are another very good overview.
Gine (1997)<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> is a rigorous and fairly self-contained theoretical
treatment of the bootstrap.</p>
<p>Although the bootstrap works in many situations, it does not always
work. For example, Abadie and Imbens (2008)<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup> show the
failure of the bootstrap for matching estimators. See Andrews
(2000)<sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup>, Andrews and Han (2009)<sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup>, and Romano
and Shaikh (2012)<sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup> for theoretical developments on
situations where the bootstrap fails and alternatives that work.  Hall
(1994)<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup> gives a theoretical overview of when the bootstrap
provides asymptotic refinement. Chernozhukov, Chetverikov, and Kato
(2017)<sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup> discusses the bootstrap in high dimensional
models.</p>
<h1 id="theory">Theory<a class="headerlink" href="#theory" title="Permanent link">&para;</a></h1>
<p>This section follows the approach Van der Vaart (2000)<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup>. We focus on the case where
$T_n = \frac{\hat{\theta}_n - \theta_0}{\hat{\sigma}_n}$ is a
t-statistic. A simple and useful result is that if $T_n$ and $T^\ast_n$
both converge to the same distribution, then the bootstrap is
consistent. </p>
<div class="admonition tip">
<p class="admonition-title"><strong>Theroem</strong>: bootstrap consistency</p>
<p>Suppose that 
<script type="math/tex; mode=display">
T_n = \frac{\hat{\theta}_n - \theta_0}{\hat{\sigma}_n} \leadsto T
</script>
and 
<script type="math/tex; mode=display">
T_n^\ast =  \frac{\hat{\theta}^\ast_n - \hat{\theta}_n}{\hat{\sigma}^\ast_n} \leadsto T
</script>
conditional on the data, for some random variable $T$ with a
continuous distribution function. Then 
<script type="math/tex; mode=display">
| G_n(\tau, F_0) - G_n(\tau,\hat{F}_n) | \inprob 0 |
</script>
and in particular,
<script type="math/tex; mode=display">
\Pr(\theta_0 \in [\hat{\theta}_n - G_n^{-1}(\alpha/2, \hat{F}_n)
\hat{\sigma}_n ,  \hat{\theta}_n - G_n^{-1}(1-\alpha/2, \hat{F}_n)
\hat{\sigma}_n ]) \to 1-\alpha.
</script>
</p>
</div>
<p><em>Proof sketch:</em> $T_n$ and $T_n^\ast$ both $\leadsto T$ immediately
implies $G_n(\tau, F_0) \inprob G_\infty(\tau)$ and
$G_n(\tau,\hat{F}<em>n) \inprob G</em>\infty(\tau)$ , where $G_\infty(\tau)$
is the CDF of $T$ . This implies that 
$G^{-1}<em>n(\tau,\hat{F}_n) \inprob G^{-1}</em>\infty(\tau)$ for all $\tau$
where $G_\infty$ is continuous. Then we have
<script type="math/tex; mode=display">
\Pr(\theta_0 \geq \hat{\theta}_n - G_n^{-1}(\tau, \hat{F}_n)
 \hat{\sigma}_n) = \Pr\left(\frac{\theta_0 -
 \hat{\theta}_n}{\hat{\sigma}_n} \leq G_n^{-1}(\tau, \hat{F}_n)\right) \to
 \Pr(T \leq G^{-1}_\infty(\tau)) = \tau.
</script>
</p>
<p>This theorem is very simple, but it is useful because it suggest a
simple path to showing the consistency of the bootstrap: simply show
that $T_n^\ast$ has the same asymptotic distribution as $T_n$. Here is a
simple result for when $T_n^\ast$ is constructed by sampling with
replacement from the empirical distribution. We will let
$\mathbb{P}_n$ denote the empirical distribution, and $x_i^\ast$ denote
draws of $x_i$ from it.</p>
<div class="admonition tip">
<p class="admonition-title"><strong>Lemma</strong></p>
<p>Let $x_1, x_2, &hellip;$ be i.i.d. with mean $\mu$ and variance
$\sigma^2$. Then conditional on $x_1, &hellip;$ for almost every sequence
<script type="math/tex; mode=display">
\sqrt{n} (\bar{x}_n^\ast - \bar{x}_n) \indist N(0,\sigma^2)
</script>
</p>
</div>
<p><strong>Proof sketch</strong> it is straightforward to show that $\Er[x_i^\ast |
\mathbb{P}_n] = \bar{x}_n$ and $Var(x_i^\ast|\mathbb{P}_n) = \bar{x^2}_n
- \bar{x}_n^2 \to \sigma^2$. Applying the Lindeberg CLT then gives the
result.</p>
<h2 id="pivotal-statistics">Pivotal statistics<a class="headerlink" href="#pivotal-statistics" title="Permanent link">&para;</a></h2>
<p>The above results imply that the bootstrap works for both 
$S_n = \sqrt{n}(\bar{x}_n - \mu_0)$ and &ldquo;studentized&rdquo; a statistic 
$T_n = \sqrt{n}(\bar{x}_n - \mu_0)/\hat{\sigma}_n$. There is some
advantage to using the later. A statistic is called pivotal if its
distribution is completely known. If we assume $x_i \sim N$, then
$T_n$ is pivotal and has a t-distribution. If we aren&rsquo;t willing to
assume normality, then the distribution of $T_n$ is unknown, but its
asymptotic distribution is completely known, $N(0,1)$. Such a
statistic is called asymptotically pivotal. $S_n$ is not
asymptotically pivotal because its asymptotic distribution depends on
the unknown variance. It is possible to show that the bootstrap
distribution of asymptotically pivotal statistics converge faster than
either the usual asymptotic approximation or the bootstrap distribution of
non-pivotal statistics. See Hall (1994)<sup id="fnref2:10"><a class="footnote-ref" href="#fn:10">10</a></sup> for details. </p>
<p>Here is a simulation to illustrate. </p>
<pre><code class="language-julia">dgp(n) = rand(Exponential(),n)
estimator(x) = mean(x)
θ0 = 1.0
N = [5 10 20 100]
B = 999
function simulatetests(n)
  function bootstrap(data, stat, S)
    n = length(data)
    T = [stat(sample(data,n)) for s in 1:S]
  end
  data = dgp(n)
  t = sqrt(n)*(mean(data)-θ0)/std(data)
  [cdf(Normal(),t),
   mean(t.&lt;bootstrap(data, d-&gt;(sqrt(n)*(mean(d) - mean(data))/std(d)), B)),
   mean((mean(data)-θ0) .&lt; bootstrap(data, d-&gt;(mean(d)-mean(data)), B))]
end
res=[hcat([simulatetests.(n) for s in 1:1000]...) for n in N]

p = 0:0.01:1
global fig = plot(layout=4, legend=:none)
for i=1:4
  plot!(fig,p, p-&gt;(mean(res[i][1,:].&lt;p)-p), title=&quot;N=$(N[i])&quot;,
        label=&quot;asymptotic&quot;, subplot=i)

  plot!(fig,p, p-&gt;(mean(res[i][2,:].&lt;p)-p), title=&quot;N=$(N[i])&quot;,
        label=&quot;pivotal bootstrap&quot;, subplot=i)
  plot!(fig,p, p-&gt;(mean(res[i][3,:].&lt;p)-p), title=&quot;N=$(N[i])&quot;,
        label=&quot;non-pivotal boostrap&quot;, subplot=i)
end
fig
</code></pre>
<p><img alt="" src="../figures/bootstrap_4_1.png" /></p>
<p>This figure shows the simulated CDF of p-values minus p. For a
perfectly sized test, the line would be identically 0. The blue lines
are for the usual t-test. Green is from bootstrapping the non-pivotal
statistic $\sqrt{n}(\bar{x}^\ast - \bar{x})$. Red is from bootstrapping a
pivotal t-statistic. As expected, the red line is closer to 0,
illustrating the advantage of bootstrapping a pivotal statistic. </p>
<h2 id="bootstrap-does-not-always-work">Bootstrap does not always work<a class="headerlink" href="#bootstrap-does-not-always-work" title="Permanent link">&para;</a></h2>
<p>It is important to remember that the bootstrap is not guaranteed to
work. A classic example is estimating the mean squared. 
Let $x_i \sim F_0$, where$F_0$ is any distribution with mean
$\mu$ and variance $\sigma^2$. The parameter of interest is $\theta = \mu^2$. The estimator
will be $\hat{\theta} = \bar{x}^2$.  The delta method and CLT imply 
<script type="math/tex; mode=display">
    \sqrt{n}(\bar{x}^2 - \mu^2) \indist 2\mu N(0,\sigma^2)
</script>
similarly conditional on the data,
$$
    \sqrt{n}(\bar{x^\ast}^2 - \bar{x}^2) \indist 2 \mu N(0,\sigma^2)$
$$
A problem occurs when $\mu=0$. The limiting distributions become point
masses at 0. The CDF is no longer continuous, so the theorem above
does not apply. </p>
<p>Here&rsquo;s an illustration </p>
<pre><code class="language-julia">function bootmeansquared(μ0, n)
  dgp(n) = rand(n) .- 0.5 .+ μ0
  estimator(x) = mean(x)^2  
  S = 1000
  T = [estimator(dgp(n)) for s in 1:S] .- μ0^2
  function bootstrap(data, estimator, S)
    n = length(data)
    θhat = estimator(data)
    [estimator(sample(data,n,replace=true)) for s in 1:S] .- θhat 
  end
  Tboot = bootstrap(dgp(n),estimator, S)
  density(T, label=&quot;true distribution&quot;)
  density!(Tboot, label=&quot;bootstrap distribution&quot;)
end
bootmeansquared(0.5,100)
</code></pre>
<p><img alt="" src="../figures/bootstrap_5_1.png" /></p>
<pre><code class="language-julia">bootmeansquared(0.0,500)
</code></pre>
<p><img alt="" src="../figures/bootstrap_6_1.png" /></p>
<p>Depending on the random numbers drawn (in particular, whether the simulated
sample mean is very close to 0 or not), the above picture may look
okay or terrible for the bootstrap. Try running it a few times to get
a sense of how bad it might be.</p>
<h1 id="bootstrap-for-gmm">Bootstrap for GMM<a class="headerlink" href="#bootstrap-for-gmm" title="Permanent link">&para;</a></h1>
<h2 id="joint-hypotheses">Joint hypotheses<a class="headerlink" href="#joint-hypotheses" title="Permanent link">&para;</a></h2>
<p>The failure of the bootstrap for the mean squared when the true mean
is 0 has important implications for GMM. In particular, the AR
statistic, </p>
<p>
<script type="math/tex; mode=display">
AR(\theta) = n [1/n \sum g_i(\theta)]' \widehat{Var}(g_i(\theta))^{-1}
[1/n \sum g_i(\theta)] 
</script>
</p>
<p>is essentially a mean squared. If we naively attempt to apply the
bootstrap by computing</p>
<p>
<script type="math/tex; mode=display">
AR(\theta)^\ast = n [1/n \sum g_i^\ast(\theta)]' \widehat{Var}(g_i^\ast(\theta))^{-1}
[1/n \sum g_i^\ast(\theta)] 
</script>
</p>
<p>we will get incorrect inference. The problem is that we want to test
$H_0: \Er[g_i(\theta)] = 0$, but in the bootstrap sample, </p>
<p>
<script type="math/tex; mode=display">
\Er[g^\ast(\theta)|data] = 1/n \sum g_i(\theta) \neq 0.
</script>
</p>
<p>For the bootstrap to work, we must ensure that the null hypothesis is
true in the bootstrap sample, we can do this by taking</p>
<p>
<script type="math/tex; mode=display">
AR(\theta)^\ast = n [1/n \sum g_i^\ast(\theta) - \bar{g}_n(\theta)]' \widehat{Var}(g_i^\ast(\theta))^{-1}
[1/n \sum g_i^\ast(\theta) - \bar{g}_n(\theta)] 
</script>
</p>
<p>where $\bar{g}_n(\theta) = 1/n \sum g_i(\theta)$. </p>
<p>Here is a simulation to illustrate. It uses the same IV-logit share
example as in the extremum estimation notes.</p>
<pre><code class="language-julia">n = 100
k = 2
iv = 3
β0 = ones(k)
π0 = vcat(I,ones(iv-k,k))
ρ = 0.5
data = IVLogitShare(n,β0,π0,ρ)

function arstat(gi)
  n = size(gi)[1]
  gn = mean(gi,dims=1) 
  W = pinv(cov(gi))
  n*( gn*W*gn')[1]
end

function ar(θ,gi)
  n,m = size(gi(θ))
  1.0-cdf(Chisq(m), arstat(gi(θ)))
end

function bootstrapAR(θ,gi)
  giθ = gi(θ)
  gn = mean(giθ,dims=1)
  n = size(giθ)[1]
  S = 999
  T =hcat([ [arstat(giθ[sample(1:n,n,replace=true),:]),
             arstat(giθ[sample(1:n,n,replace=true),:].-gn)
             ] for s in 1:S]...)'
  t = arstat(giθ)
  [1-cdf(Chisq(length(gn)),t)  mean(T.&gt;=t, dims=1)]
end

function simulatetests()
  data = IVLogitShare(n,β0,π0,ρ)
  bsp=bootstrapAR(β0, get_gi(data))
end
</code></pre>
<pre><code>simulatetests (generic function with 2 methods)
</code></pre>
<pre><code class="language-julia">pvals = vcat([simulatetests() for s in 1:500]...)
p = 0:0.01:1.0
plot(p, p-&gt;(mean(pvals[:,1].&lt;p)-p),  label=&quot;asymptotic&quot;, legend=:bottomleft)
plot!(p, p-&gt;(mean(pvals[:,2].&lt;p)-p),  label=&quot;incorrect bootstrap&quot;)
plot!(p, p-&gt;(mean(pvals[:,3].&lt;p)-p),  label=&quot;correct bootstrap&quot;)
</code></pre>
<p><img alt="" src="../figures/bootstrap_8_1.png" /></p>
<p>With the same sort of modification the KLM and CLR statistics can also
be bootstrapped. All three statistics remain identification robust
when bootstrapped. See Kleibergen (2006)<sup id="fnref:14"><a class="footnote-ref" href="#fn:14">14</a></sup> for details.</p>
<h2 id="single-coefficients">Single coefficients<a class="headerlink" href="#single-coefficients" title="Permanent link">&para;</a></h2>
<p>If we want to construct a confidence interval for a single
coefficient, we can apply the bootstrap to a statistic like 
$\sqrt{n} (\hat{\theta} - \theta_0)$ (or a studentized version of
it). Just like in the previous subsection, we must be careful to
ensure that the null hypothesis holds in the bootstrapped data. Also,
as above, we can do this by subtracting $1/n\sum_i g_i(\hat{\theta})$
from the bootstrapped moments. Thus, one way to bootstrap is 
$\sqrt{n} (\hat{\theta} - \theta_0)$ is to</p>
<ul>
<li>Compute $\hat{\theta}$, $\bar{g}_n(\theta) = 1/n\sum_i g_i(\hat{\theta})$</li>
<li>Draw with replacement $g_i^\ast(\theta)$ from ${g_i(\theta) -
  \bar{g}<em i="1">n(\theta)}</em>^n$. </li>
<li>Compute </li>
</ul>
<p>
<script type="math/tex; mode=display">\hat{\theta}^\ast = \argmin [1/n \sum_i g_i^\ast(\theta)] W_n^\ast(\theta)
  [1/n \sum g_i^\ast(\theta]</script>
</p>
<ul>
<li>Use distribution of $\sqrt{n} (\hat{\theta}^\ast - \hat{\theta})$ to
  approximate $\sqrt{n} (\hat{\theta} - \theta_0)$</li>
</ul>
<p>For some models, the minimazation needed to compute $\hat{\theta}^\ast$
can be very time consuming. Fortunately, it can be avoided. A key step
in showing that $\hat{\theta}$ is asymptotically normal is a
linearization</p>
<p>
<script type="math/tex; mode=display">
\sqrt{n}(\hat{\theta} - \theta_0) = -(D'WD)^{-1} (D' W)
\frac{1}{\sqrt{n}} \sum_i g_i(\theta_0) + o_p(1) 
</script>
</p>
<p>Showing the bootstrap estimator is asymptotically normal conditional
on the data involves a similar linearization</p>
<p>
<script type="math/tex; mode=display">
\sqrt{n}(\hat{\theta}^\ast - \hat{\theta}) = -(\hat{D}'\hat{W}\hat{D})^{-1} (\hat{D}' \hat{W})
\frac{1}{\sqrt{n}} \sum_i g_i^\ast(\hat{\theta}) + o_p(1) 
</script>
</p>
<p>This suggests taking </p>
<p>
<script type="math/tex; mode=display">
\hat{\theta}^\ast = \hat{\theta} + (\hat{D}'\hat{W}\hat{D})^{-1} (\hat{D}' \hat{W})
\frac{1}{\sqrt{n}} \sum_i g_i^\ast(\hat{\theta}) 
</script>
</p>
<p>instead of re-minimizing. </p>
<pre><code class="language-julia">function gmmVar(θ,gi,W)
  g = gi(θ)
  n = size(g)[1]
  D = ForwardDiff.jacobian(θ-&gt;mean(gi(θ),dims=1),θ)
  Σ = cov(gi(θ))
  1/n*inv(D'*W*D)*(D'*W*Σ*W*D)*inv(D'*W*D)
end

function bsweights(n)
  s = sample(1:n,n)
  (x-&gt;sum(x.==s)).(1:n)
end


function bootstrapt(θ,gi, W)
  giθ = gi(θ)
  gn = size(giθ)[1]
  D = ForwardDiff.jacobian(θ-&gt;mean(gi(θ),dims=1),θ) 
  function bootonce(θ,gi)
    giθ = gi(θ)
    n = size(giθ)[1]
    gn = mean(giθ,dims=1)
    w = bsweights(n)
    giw(θ) = (gi(θ).*w .- gn)
    gmmobj = gmm_objective(giw, W)
    opt1 = optimize(gmmobj, θ, BFGS(), autodiff =:forward)
    θs1 = opt1.minimizer
    θs2 = θ - inv(D'*W*D)*(D'*W*( (mean(giθ.*w,dims=1).-gn)' ))
    [θs1[1] θs2[1]]
  end
  S = 299
  vcat([bootonce(θ,gi) for s in 1:S]...)
end
</code></pre>
<pre><code>bootstrapt (generic function with 1 method)
</code></pre>
<pre><code class="language-julia">n = 50
k = 2
iv = 3
β0 = ones(k)
π0 = vcat(I,ones(iv-k,k))
ρ = 0.5
data = IVLogitShare(n,β0,π0,ρ)
gi = get_gi(data)
W = I
optres = optimize(gmm_objective(data,W), β0, BFGS(),
                  autodiff = :forward)
θhat = optres.minimizer
θs = bootstrapt(θhat,gi,I)
density(θs[:,1], label=&quot;minimization bootstrap&quot;)
density!(θs[:,2], label=&quot;score bootstrap&quot;)
</code></pre>
<p><img alt="" src="../figures/bootstrap_10_1.png" /></p>
<pre><code class="language-julia">function simulatetests()
  data = IVLogitShare(n,β0,π0,ρ)
  gi = get_gi(data)
  W = I
  optres = optimize(gmm_objective(gi,W), β0, BFGS(), autodiff =:forward)
  θhat = optres.minimizer 
  θs = bootstrapt(θhat,gi,I)
  p = mean((θhat[1]-β0[1]).&lt;(θs .- θhat[1]), dims=1)
end

pvals = vcat([simulatetests() for s in 1:300]...)
p = 0:0.01:1.0
fig=plot(p, p-&gt;(mean(pvals[:,1].&lt;p)-p),
         label=&quot;minimization bootstrap&quot;, legend=:best)
plot!(fig,p, p-&gt;(mean(pvals[:,2].&lt;p)-p),  label=&quot;score bootstrap&quot;)
fig
</code></pre>
<p><img alt="" src="../figures/bootstrap_11_1.png" /></p>
<p>Both versions of the bootstrap appear to work well enough here. Note
that neither one is robust to identification problems. Inference on a
subset of parameters while remaining robust to identification problems
is somewhat of an open problem. Various conservative approaches are
available, the simplest of which is to just take projections of the
AR, KLM, or CLR confidence sets. There are also more powerful
approaches for situations where you know which parameters are strongly
vs weakly identified. </p>
<h1 id="references_1">References<a class="headerlink" href="#references_1" title="Permanent link">&para;</a></h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>James G. MacKinnon. Bootstrap methods in econometrics*. <em>Economic Record</em>, 82(s1):S2&ndash;S18, 2006. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2006.00328.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2006.00328.x</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-4932.2006.00328.x">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-4932.2006.00328.x</a>, <a href="https://doi.org/10.1111/j.1475-4932.2006.00328.x">doi:10.1111/j.1475-4932.2006.00328.x</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>James G MacKinnon. Bootstrap hypothesis testing. <em>Handbook of Computational Econometrics</em>, 183:213, 2009. URL: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1033.330&amp;rep=rep1&amp;type=pdf#page=203">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1033.330&amp;rep=rep1&amp;type=pdf#page=203</a>.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Joel L. Horowitz. Chapter 52 - the bootstrap. In James J. Heckman and Edward Leamer, editors, <em>Handbook of Econometrics</em>, volume 5 of Handbook of Econometrics, pages 3159 &ndash; 3228. Elsevier, 2001. URL: <a href="http://www.sciencedirect.com/science/article/pii/S157344120105005X">http://www.sciencedirect.com/science/article/pii/S157344120105005X</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(01)05005-X">doi:https://doi.org/10.1016/S1573-4412(01)05005-X</a>.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>XiaoXia Shi. Lecture 10: bootstrap. 2012. URL: <a href="https://www.ssc.wisc.edu/~xshi/econ715/Lecture_10_bootstrap.pdf">https://www.ssc.wisc.edu/~xshi/econ715/Lecture_10_bootstrap.pdf</a>.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Evarist Gine. <em>Lectures on some aspects of the bootstrap</em>, pages 37–151. Springer Berlin Heidelberg, Berlin, Heidelberg, 1997. URL: <a href="https://doi.org/10.1007/BFb0092619">https://doi.org/10.1007/BFb0092619</a>, <a href="https://doi.org/10.1007/BFb0092619">doi:10.1007/BFb0092619</a>.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Alberto Abadie and Guido W. Imbens. On the failure of the bootstrap for matching estimators. <em>Econometrica</em>, 76(6):1537&ndash;1557, 2008. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474">https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6474</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6474">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6474</a>, <a href="https://doi.org/10.3982/ECTA6474">doi:10.3982/ECTA6474</a>.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Donald W. K. Andrews. Inconsistency of the bootstrap when a parameter is on the boundary of the parameter space. <em>Econometrica</em>, 68(2):399&ndash;405, 2000. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00114">https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00114</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00114">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00114</a>, <a href="https://doi.org/10.1111/1468-0262.00114">doi:10.1111/1468-0262.00114</a>.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Donald W. K. Andrews and Sukjin Han. Invalidity of the bootstrap and the m out of n bootstrap for confidence interval endpoints defined by moment inequalities. <em>The Econometrics Journal</em>, 12(s1):S172&ndash;S199, 2009. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1368-423X.2008.00265.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1368-423X.2008.00265.x</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1368-423X.2008.00265.x">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1368-423X.2008.00265.x</a>, <a href="https://doi.org/10.1111/j.1368-423X.2008.00265.x">doi:10.1111/j.1368-423X.2008.00265.x</a>.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Joseph P. Romano and Azeem M. Shaikh. On the uniform asymptotic validity of subsampling and the bootstrap. <em>Ann. Statist.</em>, 40(6):2798–2822, 12 2012. URL: <a href="https://doi.org/10.1214/12-AOS1051">https://doi.org/10.1214/12-AOS1051</a>, <a href="https://doi.org/10.1214/12-AOS1051">doi:10.1214/12-AOS1051</a>.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>Peter Hall. Chapter 39 methodology and theory for the bootstrap. In <em>Handbook of Econometrics</em>, volume 4 of Handbook of Econometrics, pages 2341 &ndash; 2381. Elsevier, 1994. URL: <a href="http://www.sciencedirect.com/science/article/pii/S157344120580008X">http://www.sciencedirect.com/science/article/pii/S157344120580008X</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80008-X">doi:https://doi.org/10.1016/S1573-4412(05)80008-X</a>.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in high dimensions. <em>Ann. Probab.</em>, 45(4):2309–2352, 07 2017. URL: <a href="https://doi.org/10.1214/16-AOP1113">https://doi.org/10.1214/16-AOP1113</a>, <a href="https://doi.org/10.1214/16-AOP1113">doi:10.1214/16-AOP1113</a>.&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>Aad W Van der Vaart. <em>Asymptotic statistics</em>. Volume 3. Cambridge university press, 2000.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>Peter Hall. Chapter 39 methodology and theory for the bootstrap. In <em>Handbook of Econometrics</em>, volume 4 of Handbook of Econometrics, pages 2341 &ndash; 2381. Elsevier, 1994. URL: <a href="http://www.sciencedirect.com/science/article/pii/S157344120580008X">http://www.sciencedirect.com/science/article/pii/S157344120580008X</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80008-X">doi:https://doi.org/10.1016/S1573-4412(05)80008-X</a>.&#160;<a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Frank Kleibergen. Expansions of gmm statistics and the bootstrap. 2006. URL: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.595.6481&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.595.6481&amp;rep=rep1&amp;type=pdf</a>.&#160;<a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>

        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
