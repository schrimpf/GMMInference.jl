<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Inference - GMMInference.jl</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="..">GMMInference.jl</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Package Documentation</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Notes and Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../extremumEstimation/">Estimation</a>
</li>
                                    
<li class="active">
    <a href="./">Inference</a>
</li>
                                    
<li >
    <a href="../empiricalLikelihood/">Empirical Likelihood</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../license/">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                            <li >
                                <a rel="next" href="../extremumEstimation/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../empiricalLikelihood/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/schrimpf/GMMInference.jl/edit/master/docs/identificationRobustInference.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#about-this-document">About this document</a></li>
        <li class="main "><a href="#identification-robust-inference">Identification robust inference</a></li>
            <li><a href="#anderson-rubin-test">Anderson-Rubin test</a></li>
            <li><a href="#example-iv-logit-demand">Example: IV logit demand</a></li>
            <li><a href="#other-identification-robust-tests">Other identification robust tests</a></li>
        <li class="main "><a href="#references">References</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a> </p>
<h3 -="-" id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in
<a href="https://github.com/schrimpf/GMMInference.jl">on github</a>. The same
document generates both static webpages and associated <a href="../identificationRobustInference.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
</script>
</p>
<h1 id="identification-robust-inference">Identification robust inference<a class="headerlink" href="#identification-robust-inference" title="Permanent link">&para;</a></h1>
<p>As discussed in section 9 of Newey and McFadden (1994)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, there are three classic types
of statistics for testing restrictions on parameters. Suppose you want
to test $H_0: a(\theta) = 0$. Let $\hat{\theta}$ denote the
unrestricted estimate, and let $\hat{\theta}^r$ denote the
estimate of $\theta$ subject to the restriction. Wald test-statistics
are based on $\hat{\theta}-\hat{\theta}^r$. Lagrange multiplier tests
look at the distribution of the estimated Lagrange
multiplier. Likelihood ratio (aka distance metric in Newey and McFadden (1994)<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup>) tests
look at $Q_n(\hat{\theta}) - Q_n(\hat{\theta}^r)$. If we consider
testing $H_0: \theta = \vartheta$ for some fixed $\vartheta$, then the
usual approach based on the asymptotic normality of $\hat{\theta}$
discussed above is exactly the same as the Wald test of this
restriction. As discussed by Newey and McFadden (1994)<sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, under standard assumptions,
all three testing approaches are asymptotically equivalent. However,
the tests can and will differ in finite samples. More importantly, in
the face of identification problems, Wald tests tend to break down,
while Lagrange multiplier and likelihood ratio style tests can
continue to work. </p>
<p>By identification robust, we mean an inference procedure that has
correct size regardless of whether identification is strong, weak,
or partial. In the asymptotic normality of extremum estimators theorem
above, non-strong identification will create problems for assumption
3, in particular the assumption that the Hessian is non-singular. For
this section, we will focus on GMM estimators. Identification problems
most often arrive and have been studied in the context of GMM. Also,
it is not difficult to transform other extremum estimators into GMM. </p>
<p>For a GMM objective function of the form:
<script type="math/tex; mode=display"> [1/n \sum_i g_i(\theta)] W_n [1/n \sum g_i(\theta]</script>, 
if we assume:</p>
<ol>
<li>
<p>$1/\sqrt{n} \sum_i g_i(\theta_0) \indist N(0,\Sigma)$</p>
</li>
<li>
<p>$1/n \sum_i \nabla g_i(\theta) \inprob E[\nabla g(\theta)]$, 
   $W_n \inprob W$</p>
</li>
<li>
<p>$(D&rsquo;WD)$ is nonsingular.</p>
</li>
</ol>
<p>then the above theorem for asymptotic normality of extremum
estimators implies that 
<script type="math/tex; mode=display">
\sqrt{n}(\hat{\theta} - \theta_0) \indist N(0,\Omega)
</script>
where 
<script type="math/tex; mode=display">
 \Omega= (D'WD)^{-1} (D' W \Sigma W D) (D'WD)^{-1}.
</script>
If we additionally assume $W_n \inprob \Sigma^{-1}$, e.g. observations
are independent and $W_n =
\widehat{Var}(g_i(\theta))^{-1}$, then the asymptotic variance
simplifies to $(D&rsquo; \Sigma D)^{-1}$. </p>
<h3 id="anderson-rubin-test">Anderson-Rubin test<a class="headerlink" href="#anderson-rubin-test" title="Permanent link">&para;</a></h3>
<p>As already stated, the assumption that $(D&rsquo;WD)$ is nonsingular is
problematic if we want to allow for identification problems. However,
if we assume only that </p>
<ol>
<li>
<p>$1/\sqrt{n} \sum_i g_i(\theta_0) \indist N(0,\Sigma)$</p>
</li>
<li>
<p>$W_n \inprob \Sigma^{-1}$</p>
</li>
</ol>
<p>then 
<script type="math/tex; mode=display">
n [1/n \sum g_i(\theta_0)]' W_n [1/n \sum g_i(\theta_0)]
  \indist \chi^2_m
</script>
where $m$ is the number of moments (dimension of $g_i(\theta)$). This
is called the Anderson-Rubin test. Note that this result holds without
any explicit nonsingular assumption about a Hessian. Hence, there is
hope that this result would be true even with identification
problems. Indeed, it is. Stock and Wright (2000)<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> first proposed using this test
statistic for weakly identified GMM estimators. Stock, Wright, and Yogo (2002)<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> gives an
overview of this test and related tests with a focus on linear
IV. Caner (2009)<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> discusses this test in the context of GMM.</p>
<p>Typical usage of the AR test is to invert the test to construct a
confidence region for $\theta$. For each $\theta \in \Theta$, 
let 
<script type="math/tex; mode=display">
AR(\theta) = n [1/n \sum g_i(\theta)]' \widehat{Var}(g_i(\theta))^{-1}
[1/n \sum g_i(\theta)] 
</script>
and let $c_{\alpha}= \alpha$ quantile of $\chi^2_m$. Then a $\alpha$
confidence region for $\theta_0$ is
<script type="math/tex; mode=display">
\{ \theta \in \Theta: AR(\theta) \leq c_\alpha \}
</script>
</p>
<h3 id="example-iv-logit-demand">Example: IV logit demand<a class="headerlink" href="#example-iv-logit-demand" title="Permanent link">&para;</a></h3>
<p>A common way to model demand for differentiated products is to
aggregate an individual discrete choice. We will look at the simplest
such model here. This is a model for when we have data on product
market shares, $y_j$, and product attributes, $x_j$, for many
different markets. In concrete applications, markets may be defined
geographically, temporally, by consumer segment, or some combination
thereof. </p>
<p>Consider a single good, which consumers chooose to
purchase or not. Consumer $i$&rsquo;s utility from consuming the good is
<script type="math/tex; mode=display">
u_{ij} = x_j \beta + \xi_j + \epsilon_{ij}
</script>
where $x_j$ are the observed attributes of the good in market $j$,
$\xi_j$ is a market level demand shock, and $\epsilon_{ij}$ is an
individual taste shock. Person $i$ purchases the good if 
$u_{ij} \geq 0$. Aggregating individual purchases implies that the
market share in market $j$ is 
<script type="math/tex; mode=display"> y_j = F_{-\epsilon}(x_j \beta + \xi_j) </script>
where $F_{-\epsilon}$ is the CDF of $-\epsilon$.</p>
<p>We assume that $\epsilon_{ij}$ is independent of $x_j$ and
$\xi_j$. Typically, $x_j$ includes some easily adjusted product
attributes, such as price, so we want to allow $x_j$ to be correlated
with $\xi_j$. Assume that we have some instruments $z_j$ such that
$\Er[\xi_j z_j]=0.$  We can write this moment condition in terms of
observables and $\beta$ as
<script type="math/tex; mode=display"> \Er[ (F^{-1}_{-\epsilon}(y_j) - x_j\beta) z_j ] = 0 </script>
This is the moment condition we will use to estimate $\beta$.</p>
<p>First, we will simulate the model, then estimate it. This code looks
at three variants of GMM. </p>
<p>First, it computes an estimate with $W_n = I$. </p>
<p>Second, it computes an efficiently weighted estimated with 
<script type="math/tex; mode=display">
W_n = \widehat{Var}(g_i(\hat{\theta}_{(1)}))
</script>
where
$\hat{\theta}_{(1)}$ is the first estimate. </p>
<p>Third, it computes the continuous updating estimator, which uses
$AR(\theta)$ as the objective function ( $W$ is &ldquo;continuously updated&rdquo;
to be $\widehat{Var}( g_i(\theta))$ ).</p>
<pre><code class="julia">using Optim, ForwardDiff, LinearAlgebra, Distributions
function simulate_ivshare(n,β,γ,ρ)
  z = randn(n, size(γ)[1])
  endo = randn(n, length(β))
  x = z*γ .+ endo
  ξ = rand(Normal(0,sqrt((1.0-ρ^2))),n).+endo[:,1]*ρ 
  y = cdf.(Logistic(), x*β .+ ξ)
  return((y=y,x=x,z=z))  
end
n = 100
k = 2
iv = 3
β0 = ones(k)
π0 = vcat(5*I,ones(iv-k,k)) 
ρ = 0.5  
(y,x,z) = simulate_ivshare(n,β0,π0,ρ)

function gi_ivshare(β,y,x,z)
  ξ = quantile.(Logistic(),y) .- x*β
  ξ.*z
end

function gmmObj(θ,gi,W)
  g = gi(θ)
  m = mean(g,dims=1)
  (size(g)[1]*( m*W*m')[1]) # return scalar, not 1x1 array
end

function gmmVar(θ,gi,W)
  g = gi(θ)
  n = size(g)[1]
  D = ForwardDiff.jacobian(θ-&gt;mean(gi(θ),dims=1),θ)
  Σ = cov(gi(θ))
  1/n*inv(D'*W*D)*(D'*W*Σ*W*D)*inv(D'*W*D)
end

function ar(θ,gi)
  gmmObj(θ,gi,inv(cov(gi(θ))))
end
</code></pre>

<pre><code>ar (generic function with 1 method)
</code></pre>

<pre><code class="julia">opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                zeros(k), BFGS(), autodiff =:forward)
@show β1 = opt1.minimizer
</code></pre>

<pre><code>β1 = opt1.minimizer = [1.0182986797223397, 1.0089485255913304]
</code></pre>

<pre><code class="julia">display(gmmVar(β1, β-&gt;gi_ivshare(β,y,x,z),I))
</code></pre>

<pre><code>2×2 Array{Float64,2}:
  0.000447813  -7.09023e-5 
 -7.09023e-5    0.000394736
</code></pre>

<pre><code class="julia">opteff = optimize(θ-&gt;gmmObj(θ,β-&gt;gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(β1,y,x,z)))),
                  zeros(k), BFGS(), autodiff =:forward)
@show βeff = opteff.minimizer
</code></pre>

<pre><code>βeff = opteff.minimizer = [1.0183281895476939, 1.008985471629651]
</code></pre>

<pre><code class="julia">display(gmmVar(βeff,β-&gt;gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(β1,y,x,z)))))
</code></pre>

<pre><code>2×2 Array{Float64,2}:
  0.000447421  -7.14111e-5
 -7.14111e-5    0.00039407
</code></pre>

<pre><code class="julia">
ar_ivshare = θ-&gt;ar(θ,β-&gt;gi_ivshare(β,y,x,z))
optcue = optimize(ar_ivshare,
                  β0, BFGS(), autodiff =:forward)
@show βcue = optcue.minimizer
</code></pre>

<pre><code>βcue = optcue.minimizer = [1.0183288153534862, 1.008984973441169]
</code></pre>

<pre><code class="julia">Vcue = gmmVar(βcue,β-&gt;gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))
display(Vcue)
</code></pre>

<pre><code>2×2 Array{Float64,2}:
  0.000447421  -7.14111e-5
 -7.14111e-5    0.00039407
</code></pre>

<p>Now we compare confidence regions based on the Wald test, and from
inverting the AR statistic.</p>
<pre><code class="julia">using Plots, LaTeXStrings
Plots.gr()
function plot_cr(β,V, AR)
  lb = β - sqrt.(diag(V))*5
  ub = β + sqrt.(diag(V))*5
  ntest = 1000
  βtest = [rand(length(β)).*(ub-lb) .+ lb for i in 1:ntest]
  arstat = AR.(βtest)
  βtest = vcat(βtest'...)
  crit = quantile(Chisq(size(z)[2]), 0.9)
  scatter(βtest[:,1],βtest[:,2], group=(arstat.&lt;crit), legend=false,
          markersize=4, markerstrokewidth=0.0, seriesalpha=0.8,
          xlabel=L&quot;\beta_1&quot;, ylabel=L&quot;\beta_2&quot;)
  scatter!([β0[1]], [β0[2]], markersize=8)
  b1 = lb[1]:(ub[1]-lb[1])/100:ub[1]
  b2 = lb[2]:(ub[2]-lb[2])/100:ub[2]
  arf = (a,b) -&gt; cdf(Chisq(size(z)[2]),AR([a,b]))
  contour!(b1,b2,arf, levels = [0.9, 0.95],
           contour_labels=false, legend=false,
           label=&quot;AR CI&quot;,
           c=cgrad([:black,:black],[0.0,1.0]))
  waldf = (a,b) -&gt; cdf(Chisq(length(βcue)),([a,b]-βcue)'*inv(Vcue)*([a,b]-βcue))
  contour!(b1,b2,waldf, levels = [0.9, 0.95],
           contour_labels=false,
           label=&quot;Wald CI&quot;, c=cgrad([:red,:red], [0.0,1.0]),
           legend=false)
end
plot_cr(βcue,Vcue, ar_ivshare)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_4_1.png" /></p>
<p>The two confidence regions above are not too different because the
simulated data was strongly identified. Let&rsquo;s see what happens when we
change the simulation to have weaker identification.</p>
<pre><code class="julia"># make π0 nearly rank-deficient
π0 = ones(iv,k) .+ vcat(I*0.05,zeros(iv-k,k))  
ρ = 0.5  
(y,x,z) = simulate_ivshare(n,β0,π0,ρ)
ar_ivshare = θ-&gt;ar(θ,β-&gt;gi_ivshare(β,y,x,z))
optcue = optimize(ar_ivshare,
                  β0, BFGS(), autodiff =:forward)
@show βcue = optcue.minimizer
</code></pre>

<pre><code>βcue = optcue.minimizer = [0.5961808788571339, 1.4817274296498473]
</code></pre>

<pre><code class="julia">Vcue = gmmVar(βcue,β-&gt;gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))
display(Vcue)
</code></pre>

<pre><code>2×2 Array{Float64,2}:
  0.443472  -0.479362
 -0.479362   0.524355
</code></pre>

<pre><code class="julia">plot_cr(βcue,Vcue, ar_ivshare)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_5_1.png" /></p>
<p>Now the confidence regions are dramatically different. Does either one
have correct coverage? Let&rsquo;s simulate to find the size of the AR and
Wald tests of $H_0 : \beta = \beta_0$</p>
<pre><code class="julia">S = 500
n = 100
function sim_p(π0)
  (y,x,z) = simulate_ivshare(n,β0,π0,ρ)
  opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                  β0, BFGS(), autodiff =:forward)
  if (!opt1.g_converged)
    opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                    β0, NewtonTrustRegion(), autodiff =:forward)
  end
  β1 = opt1.minimizer

  V1 = gmmVar(β1,β-&gt;gi_ivshare(β,y,x,z),I)

  optcue = optimize(θ-&gt;ar(θ,β-&gt;gi_ivshare(β,y,x,z)),
                    β0, BFGS(), autodiff =:forward)
  if (!optcue.g_converged)
    optcue = optimize(θ-&gt;ar(θ,β-&gt;gi_ivshare(β,y,x,z)),
                      β0, NewtonTrustRegion(), autodiff =:forward)
  end
  if (!optcue.g_converged)
    display(optcue)
    βcue = deepcopy(β1)
  else 
    βcue = optcue.minimizer
  end
  Vcue =
    gmmVar(βcue,β-&gt;gi_ivshare(β,y,x,z),inv(cov(gi_ivshare(βcue,y,x,z))))
  arp = θ-&gt;(1.0-cdf(Chisq(size(z)[2]),ar(θ,β-&gt;gi_ivshare(β,y,x,z))))
  waldp = θ-&gt;(1.0-cdf(Chisq(length(βcue)),(θ-βcue)'*inv(Vcue)*(θ-βcue)))
  waldp1 = θ-&gt;(1.0-cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1)))
  [arp(β0) waldp(β0) waldp1(β0)]
end
πweak = ones(iv,k) .+ vcat(diagm(0=&gt;fill(0.01,k)),zeros(iv-k,k))  
πstrong = vcat(5*diagm(0=&gt;ones(k)),ones(iv-k,k)) 
@time pweak=vcat([sim_p(πweak) for s in 1:S]...)
</code></pre>

<pre><code>* Status: failure (reached maximum number of iterations) (line search fail
ed)

 * Candidate solution
    Minimizer: [-5.38e+03, 6.07e+03]
    Minimum:   9.538249e-01

 * Found with
    Algorithm:     Newton's Method (Trust Region)
    Initial Point: [1.00e+00, 1.00e+00]

 * Convergence measures
    |x - x'|               = 2.06e+00 ≰ 0.0e+00
    |x - x'|/|x'|          = 3.40e-04 ≰ 0.0e+00
    |f(x) - f(x')|         = 7.61e-08 ≰ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 7.98e-08 ≰ 0.0e+00
    |g(x)|                 = 2.06e-08 ≰ 1.0e-08

 * Work counters
    Seconds run:   1  (vs limit Inf)
    Iterations:    1000
    f(x) calls:    1001
    ∇f(x) calls:   1001
    ∇²f(x) calls:  1001

 * Status: failure (reached maximum number of iterations) (line search fail
ed)

 * Candidate solution
    Minimizer: [-4.83e+03, 4.53e+03]
    Minimum:   2.466659e-01

 * Found with
    Algorithm:     Newton's Method (Trust Region)
    Initial Point: [1.00e+00, 1.00e+00]

 * Convergence measures
    |x - x'|               = 1.64e+00 ≰ 0.0e+00
    |x - x'|/|x'|          = 3.39e-04 ≰ 0.0e+00
    |f(x) - f(x')|         = 5.05e-08 ≰ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 2.05e-07 ≰ 0.0e+00
    |g(x)|                 = 1.64e-08 ≰ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    1000
    f(x) calls:    1001
    ∇f(x) calls:   1001
    ∇²f(x) calls:  1001

 * Status: failure (reached maximum number of iterations) (line search fail
ed)

 * Candidate solution
    Minimizer: [-6.62e+03, 5.83e+03]
    Minimum:   1.156521e+00

 * Found with
    Algorithm:     Newton's Method (Trust Region)
    Initial Point: [1.00e+00, 1.00e+00]

 * Convergence measures
    |x - x'|               = 2.26e+00 ≰ 0.0e+00
    |x - x'|/|x'|          = 3.41e-04 ≰ 0.0e+00
    |f(x) - f(x')|         = 9.03e-08 ≰ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 7.81e-08 ≰ 0.0e+00
    |g(x)|                 = 2.26e-08 ≰ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    1000
    f(x) calls:    1001
    ∇f(x) calls:   1001
    ∇²f(x) calls:  1001

 * Status: failure (reached maximum number of iterations) (line search fail
ed)

 * Candidate solution
    Minimizer: [-6.99e+03, 7.74e+03]
    Minimum:   1.065352e+00

 * Found with
    Algorithm:     Newton's Method (Trust Region)
    Initial Point: [1.00e+00, 1.00e+00]

 * Convergence measures
    |x - x'|               = 2.64e+00 ≰ 0.0e+00
    |x - x'|/|x'|          = 3.42e-04 ≰ 0.0e+00
    |f(x) - f(x')|         = 1.27e-07 ≰ 0.0e+00
    |f(x) - f(x')|/|f(x')| = 1.19e-07 ≰ 0.0e+00
    |g(x)|                 = 2.64e-08 ≰ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    1000
    f(x) calls:    1001
    ∇f(x) calls:   1001
    ∇²f(x) calls:  1001

 12.669214 seconds (22.73 M allocations: 2.407 GiB, 26.49% gc time)
</code></pre>

<pre><code class="julia">@time pstrong=vcat([sim_p(πstrong) for s in 1:S]...)
</code></pre>

<pre><code>1.235078 seconds (1.08 M allocations: 743.800 MiB, 40.82% gc time)
</code></pre>

<pre><code class="julia">
pgrid = 0:0.01:1
plot(pgrid, p-&gt;mean( pstrong[:,1] .&lt;= p), legend=:topleft,
     label=&quot;AR, strong ID&quot;, style=:dash, color=:red,
     xlabel=&quot;p&quot;, ylabel=&quot;P(p value &lt; p)&quot;,
     title=&quot;Simulated CDF of p-values&quot;) 
plot!(pgrid, p-&gt;mean( pstrong[:,2] .&lt;= p),
      label=&quot;Wald CUE, strong ID&quot;, style=:dash, color=:blue)
plot!(pgrid, p-&gt;mean( pstrong[:,3] .&lt;= p),
      label=&quot;Wald I, strong ID&quot;, style=:dash, color=:green)

plot!(pgrid, p-&gt;mean( pweak[:,1] .&lt;= p),
      label=&quot;AR, weak ID&quot;, style=:solid, color=:red)
plot!(pgrid, p-&gt;mean( pweak[:,2] .&lt;= p),
      label=&quot;Wald CUE, weak ID&quot;, style=:solid, color=:blue)
plot!(pgrid, p-&gt;mean( pweak[:,3] .&lt;= p),
      label=&quot;Wald I, weak ID&quot;, style=:solid, color=:green)

plot!(pgrid,pgrid,alpha=0.1, label=&quot;&quot;)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_6_1.png" /></p>
<p>We see that the Wald test has fairly large size distortions, even when
identification is strong. The AR test has approximately correct size
for both the weakly and strongly identified DGP. </p>
<h3 id="other-identification-robust-tests">Other identification robust tests<a class="headerlink" href="#other-identification-robust-tests" title="Permanent link">&para;</a></h3>
<p>There are also identification robust versions of likelihood ratio and
lagrange multiplier test. Moreire (2003)<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> proposed a conditional
likelihood ratio (CLR) test for weakly identified linear IV
models. Kleibergen (2005)<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup> developed a Lagrange multiplier (often called
the KLM) test and extended Moreira&rsquo;s CLR test to weakly identified GMM
models.  More recently, Andrews and Guggenberge (2015) <sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup>
and Andrews and Guggenberge (2017) <sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup> showed the
validity of these tests under more general conditions. These tests are
somewhat more complicated than the AR test, but they have the
advantage that they are often more powerful. The AR test statistic has
a $\chi^2_{m}$ distribution, where $m$ is the number of moments. The
CLR and KLM statistics under strong identification have $\chi^2_k$
distributions (as does the Wald statistic), where $k$ is the number of
parameters. Consequently, when the model is overidentified, the CLR
and LM tests are more powerful than the AR test. </p>
<p>Here is an implementation of the KLM and CLR statistics. The names of
variables roughly follows the notation of Andrews and
Guggenberge(2017) <sup id="fnref2:8"><a class="footnote-ref" href="#fn:8">8</a></sup>.</p>
<pre><code class="julia">using ForwardDiff, Plots, Optim
Plots.gr()
function statparts(θ,gi)
  # compute common components of klm, rk, &amp; clr stats
  # follows notation of Andrews &amp; Guggenberger 2017, section 3.1
  function P(A::AbstractMatrix) # projection matrix
    A*pinv(A'*A)*A'
  end
  giθ = gi(θ)
  p = length(θ)    
  (n, k) = size(giθ)
  Ω = cov(giθ)  
  gn=mean(gi(θ), dims=1)'
  #G = ForwardDiff.jacobian(θ-&gt;mean(gi(θ),dims=1),θ)
  Gi= ForwardDiff.jacobian(gi,θ)
  Gi = reshape(Gi, n , k, p)
  G = mean(Gi, dims=1)
  Γ = zeros(eltype(G),p,k,k)
  D = zeros(eltype(G),k, p)
  for j in 1:p
    for i in 1:n
      Γ[j,:,:] += (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'
    end
    Γ[j,:,:] ./= n
    D[:,j] = G[1,:,j] - Γ[j,:,:]*inv(Ω)*gn
  end
  return(n,k,p,gn, Ω, D, P)
end

function klm(θ,gi)
  (n,k,p,gn, Ω, D, P) = statparts(θ,gi)
  lm = n*(gn'*Ω^(-1/2)*P(Ω^(-1/2)*D)*Ω^(-1/2)*gn)[1]
end

function clr(θ,gi)
  (n,k,p,gn, Ω, D, P) = statparts(θ,gi)

  rk = eigmin(n*D'*inv(Ω)*D)
  AR  = (n*gn'*inv(Ω)*gn)[1]
  lm = (n*(gn'*Ω^(-1/2)*P(Ω^(-1/2)*D)*Ω^(-1/2)*gn))[1]  
  lr = 1/2*(AR - rk + sqrt( (AR-rk)^2 + 4*lm*rk))

  # simulate to find p-value
  S = 5000
  function randc(k,p,r,S)
    χp = rand(Chisq(p),S)
    χkp = rand(Chisq(k-p),S)
    0.5.*(χp .+ χkp .- r .+
          sqrt.((χp .+ χkp .- r).^2 .+ 4 .* χp.*r))
  end
  csim = randc(k,p,rk,S)
  pval = mean(csim.&lt;=lr)
end
</code></pre>

<pre><code>clr (generic function with 1 method)
</code></pre>

<pre><code class="julia">function plot_cr(β,V, tests::AbstractArray{Function}, labels; ngrid=30)
  lb = β - sqrt.(diag(V))*5
  ub = β + sqrt.(diag(V))*5
  fig=scatter([β0[1]], [β0[2]], markersize=8, legend=false,
              xlabel=L&quot;\beta_1&quot;, ylabel=L&quot;\beta_2&quot;)
  ntest = 1000
  βtest = [rand(2).*(ub-lb) .+ lb for i in 1:ntest]
  pval = tests[1].(βtest)
  βtest = vcat(βtest'...)
  crit = 0.9
  fig=scatter!(βtest[:,1],βtest[:,2], group=(pval.&lt;crit), legend=false,
               markersize=4, markerstrokewidth=0.0, seriesalpha=0.5,
               palette=:heat)
  b1 = lb[1]:(ub[1]-lb[1])/ngrid:ub[1]
  b2 = lb[2]:(ub[2]-lb[2])/ngrid:ub[2]
  colors = [:black, :red, :blue, :green]
  for t in 1:length(tests)
    fig=contour!(b1,b2,(a,b)-&gt;tests[t]([a,b]),
             levels = [0.9, 0.95],
             contour_labels=false, legend=false,
             label = labels[t],
             c=cgrad([colors[t],colors[t]],[0.0,1.0]))
  end
  fig
end
</code></pre>

<pre><code>plot_cr (generic function with 2 methods)
</code></pre>

<p>Here&rsquo;s what the confidence regions look like when identification is
fairly weak. The green lines are the Wald confidence region, blue is
AR, red is KLM, and black is CLR. </p>
<pre><code class="julia">n = 50
k = 2
iv =3 
π0 = vcat(0.1*diagm(0=&gt;ones(k)),0.2*ones(iv-k,k)) 
ρ = 0.5  
(y,x,z) = simulate_ivshare(n,β0,π0,ρ)
opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                β0, BFGS(), autodiff =:forward)
β1 = opt1.minimizer
V1 = gmmVar(β1,β-&gt;gi_ivshare(β,y,x,z),I)


pklm = θ-&gt;cdf(Chisq(length(βcue)),klm(θ, β-&gt;gi_ivshare(β,y,x,z)))
par  = θ-&gt;cdf(Chisq(size(z)[2]), ar(θ, β-&gt;gi_ivshare(β,y,x,z)))
pclr  = θ-&gt;clr(θ, β-&gt;gi_ivshare(β,y,x,z))
pwald = θ -&gt; cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))
plot_cr(β1,V1, [pclr, pklm, par, pwald],
        [&quot;CLR&quot;,&quot;KLM&quot;,&quot;AR&quot;,&quot;Wald&quot;], ngrid=40)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_9_1.png" /></p>
<p>Here&rsquo;s what the confidence regions look like when identification is
stronger.</p>
<pre><code class="julia">n = 50
k = 2
iv =3 
π0 = vcat(3*diagm(0=&gt;ones(k)),ones(iv-k,k)) 
ρ = 0.5  
(y,x,z) = simulate_ivshare(n,β0,π0,ρ)
opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                β0, BFGS(), autodiff =:forward)
β1 = opt1.minimizer
V1 = gmmVar(β1,β-&gt;gi_ivshare(β,y,x,z),I)


pklm = θ-&gt;cdf(Chisq(length(βcue)),klm(θ, β-&gt;gi_ivshare(β,y,x,z)))
par  = θ-&gt;cdf(Chisq(size(z)[2]), ar(θ, β-&gt;gi_ivshare(β,y,x,z)))
pclr  = θ-&gt;clr(θ, β-&gt;gi_ivshare(β,y,x,z))
pwald = θ -&gt; cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))
plot_cr(β1,V1, [pclr, pklm, par, pwald],
        [&quot;CLR&quot;,&quot;KLM&quot;,&quot;AR&quot;,&quot;Wald&quot;], ngrid=40)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_10_1.png" /></p>
<p>Check the size</p>
<pre><code class="julia">S = 300
n = 100
function sim_p(π0)
  (y,x,z) = simulate_ivshare(n,β0,π0,ρ)
  opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                  β0, BFGS(), autodiff =:forward)
  if (!opt1.g_converged)
    opt1 = optimize(θ-&gt;gmmObj(θ, β-&gt;gi_ivshare(β,y,x,z) ,I),
                    β0, NewtonTrustRegion(), autodiff =:forward)
  end
  β1 = opt1.minimizer
  V1 = gmmVar(β1,β-&gt;gi_ivshare(β,y,x,z),I)

  pklm = θ-&gt;cdf(Chisq(length(β1)),klm(θ, β-&gt;gi_ivshare(β,y,x,z)))
  par  = θ-&gt;cdf(Chisq(size(z)[2]), ar(θ, β-&gt;gi_ivshare(β,y,x,z)))
  pclr  = θ-&gt;clr(θ, β-&gt;gi_ivshare(β,y,x,z))
  pwald = θ -&gt; cdf(Chisq(length(β1)),(θ-β1)'*inv(V1)*(θ-β1))  
  [par(β0) pwald(β0) pclr(β0) pklm(β0)]
end
πweak = ones(iv,k) .+ vcat(diagm(0=&gt;fill(0.01,k)),zeros(iv-k,k))  
πstrong = vcat(3*diagm(0=&gt;ones(k)),ones(iv-k,k)) 
pweak=vcat([sim_p(πweak ) for s in 1:S]...)
pstrong=vcat([sim_p(πstrong) for s in 1:S]...)

pgrid = 0:0.01:1
plot(pgrid, p-&gt;mean( pstrong[:,1] .&lt;= p), legend=:topleft,
     label=&quot;AR, strong ID&quot;, style=:dash, color=:blue,
     xlabel=&quot;p&quot;, ylabel=&quot;P(p value &lt; p)&quot;,
     title=&quot;Simulated CDF of p-values&quot;) 
plot!(pgrid, p-&gt;mean( pstrong[:,2] .&lt;= p),
      label=&quot;Wald I, strong ID&quot;, style=:dash, color=:green)
plot!(pgrid, p-&gt;mean( pstrong[:,3] .&lt;= p),
      label=&quot;CLR, strong ID&quot;, style=:dash, color=:black)
plot!(pgrid, p-&gt;mean( pstrong[:,4] .&lt;= p),
      label=&quot;KLM, strong ID&quot;, style=:dash, color=:red)

plot!(pgrid, p-&gt;mean( pweak[:,1] .&lt;= p),
      label=&quot;AR, weak ID&quot;, style=:solid, color=:blue)
plot!(pgrid, p-&gt;mean( pweak[:,2] .&lt;= p),
      label=&quot;Wald I, weak ID&quot;, style=:solid, color=:green)
plot!(pgrid, p-&gt;mean( pweak[:,3] .&lt;= p),
      label=&quot;CLR, weak ID&quot;, style=:solid, color=:black)
plot!(pgrid, p-&gt;mean( pweak[:,4] .&lt;= p),
      label=&quot;KLM, weak ID&quot;, style=:solid, color=:red)
plot!(pgrid,pgrid,alpha=0.5, label=&quot;&quot;)
</code></pre>

<p><img alt="" src="../figures/identificationRobustInference_11_1.png" /></p>
<!-- ## Bootstrap -->

<!-- The bootstrap and related simulation methods can also be used for -->

<!-- inference. We will look at these in more detail in the next set of notes. -->

<!-- ## Bayesian methods -->

<!-- Bayesian methods give an alternative approach to both estimation and -->

<!-- inference. In some situations, Bayesian methods can be more convenient -->

<!-- and/or more numerically robust. In well behaved settings, when viewed -->

<!-- from a frequentist perspective, Bayesian methods asymptotically give -->

<!-- the same results as MLE (or efficiently weighted GMM in the case of -->

<!-- quasi-Bayesian models).  -->

<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Whitney K. Newey and Daniel McFadden. Chapter 36 large sample estimation and hypothesis testing. In <em>Handbook of Econometrics</em>, volume 4 of Handbook of Econometrics, pages 2111 &ndash; 2245. Elsevier, 1994. URL: <a href="http://www.sciencedirect.com/science/article/pii/S1573441205800054">http://www.sciencedirect.com/science/article/pii/S1573441205800054</a>, <a href="https://doi.org/https://doi.org/10.1016/S1573-4412(05)80005-4">doi:https://doi.org/10.1016/S1573-4412<script type="math/tex">05</script>80005-4</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>James H. Stock and Jonathan H. Wright. Gmm with weak identification. <em>Econometrica</em>, 68<script type="math/tex">5</script>:1055&ndash;1096, 2000. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00151">https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00151</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00151">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00151</a>, <a href="https://doi.org/10.1111/1468-0262.00151">doi:10.1111/1468-0262.00151</a>.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>James H Stock, Jonathan H Wright, and Motohiro Yogo. A survey of weak instruments and weak identification in generalized method of moments. <em>Journal of Business &amp; Economic Statistics</em>, 20<script type="math/tex">4</script>:518&ndash;529, 2002. URL: <a href="https://doi.org/10.1198/073500102288618658">https://doi.org/10.1198/073500102288618658</a>, <a href="https://arxiv.org/abs/https://doi.org/10.1198/073500102288618658">arXiv:https://doi.org/10.1198/073500102288618658</a>, <a href="https://doi.org/10.1198/073500102288618658">doi:10.1198/073500102288618658</a>.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Mehmet Caner. Testing, estimation in gmm and cue with nearly-weak identification. <em>Econometric Reviews</em>, 29<script type="math/tex">3</script>:330&ndash;363, 2009. URL: <a href="https://doi.org/10.1080/07474930903451599">https://doi.org/10.1080/07474930903451599</a>, <a href="https://arxiv.org/abs/https://doi.org/10.1080/07474930903451599">arXiv:https://doi.org/10.1080/07474930903451599</a>, <a href="https://doi.org/10.1080/07474930903451599">doi:10.1080/07474930903451599</a>.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Marcelo J. Moreira. A conditional likelihood ratio test for structural models. <em>Econometrica</em>, 71<script type="math/tex">4</script>:1027&ndash;1048, 2003. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00438">https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00438</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00438">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00438</a>, <a href="https://doi.org/10.1111/1468-0262.00438">doi:10.1111/1468-0262.00438</a>.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Frank Kleibergen. Testing parameters in gmm without assuming that they are identified. <em>Econometrica</em>, 73<script type="math/tex">4</script>:1103&ndash;1123, 2005. URL: <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00610.x">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2005.00610.x</a>, <a href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2005.00610.x">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2005.00610.x</a>, <a href="https://doi.org/10.1111/j.1468-0262.2005.00610.x">doi:10.1111/j.1468-0262.2005.00610.x</a>.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Donald Andrews and Patrik Guggenberger. Identification-and singularity-robust inference for moment condition models. 2015. URL: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2545374">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2545374</a>.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Donald W.K. Andrews and Patrik Guggenberger. Asymptotic size of kleibergen’s lm and conditional lr tests for moment condition models. <em>Econometric Theory</em>, 33<script type="math/tex">5</script>:1046–1080, 2017. <a href="https://doi.org/10.1017/S0266466616000347">doi:10.1017/S0266466616000347</a>.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>

        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
